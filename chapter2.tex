\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,amsthm,tikz,hyperref}
\usepackage{bbm}

\newtheorem{exercise}{Exercise}[subsection]

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\cN}{\mathcal{N}}
\newcommand*{\Var}{\mathrm{Var}}
\newcommand*{\Cov}{\mathrm{Cov}}
\newcommand*{\esssup}{\mathrm{ess}\,\sup}
\newcommand*{\ind}[1]{\mathbbm{1}_{[{#1}]}}
\newcommand*{\defeq}{\stackrel{\mathrm{def}}{=}}

\renewcommand{\theenumi}{(\alph{enumi})}
\makeatletter

\newenvironment{customExercise}[1]
 {\count@\c@exercise
 \global\c@exercise#1 %
   \global\advance\c@exercise\m@ne
   \exercise}
  {\endexercise
  \global\c@exercise\count@}

\title{Concentration of Sums of Independent Random Variables}
\author{Taeyoung Kim}
\date{\today}

\begin{document}
\maketitle

\setcounter{section}{2}
\setcounter{subsection}{1}

\begin{customExercise}{4}
  Let $g \sim \cN(0, 1)$. Show that, for all $t \ge 1$, we have
  \[\E g^2 \ind{g > t} = t \frac{1}{\sqrt{2\pi}} e^{-t^2/2} + P\left( g > t \right) \le \left( t + \frac{1}{t}\right) \frac{1}{\sqrt{2\pi}} e^{-t^2/2}.\]
\end{customExercise}
\begin{proof}
  We use the following equality
  \[
    x^2 e^{-x^2/2} = e^{-x^2/2} - \frac{d}{dx} \left( x e^{-x^2/2} \right)
  \]
  which gives
  \begin{align*}
    &\E g^2 \ind{g > t} 
    \\
    = &\int_t^\infty g^2 e^{-g^2 / 2} dg 
    \\
    = &\int_t^\infty \left(e^{-g^2/2} - \frac{d}{dg} \left( x e^{-x^2/2} \right) \right)dg
    \\
    = &P(g > t) - \left(\lim_{x \to \infty} \left(x e^{-x^2/2}\right) - t e^{-t^2/2}\right)
    \\
    = &P(g > t) + t e^{-t^2/2}.
  \end{align*}
  The inequality directly follows from Proposition 2.1.3.
\end{proof}

\setcounter{subsection}{2}

\begin{customExercise}{3}
  Show that
  \[\cosh(x) = \frac{e^x + e^{-x}}{2} \le \exp(x^2/2)\]
  for all $x \in \R$.  
\end{customExercise}
\begin{proof}
  Take logarithm to both side, and subtract LHS from RHS. This gives
  \[x^2/2 - \log (\cosh(x))\]
  which is zero at $x=0$.
  Now differentiate this, which gives
  \[x - \tanh(x)\]
  so it is enough to show that 
  \[\tanh(x) \begin{cases}
    \le x & x > 0\\
    \ge x & x < 0
  \end{cases}\]
  Showing the first condition shows the second, since $\tanh$ is symmetric at zero.
  Now differentiate $\tanh$, we have $\tanh'(x) = 1 - \tanh^2(x)$.
  Then for $x > 0$
  \[\tanh(x) = \int_0^x (1 - \tanh^2(t)) dt \le \int_0^x 1 dt = x\]
  finishing the proof. 
  
\end{proof}

\begin{customExercise}{7}
  Prove Theorem 2.2.6, possibly with some absolute constant instead of 2 in the tail.
\end{customExercise}
\begin{proof}
  We will first assume that $\E X_i = 0$ for all $i$, since it does not change both sides.

  Let's take similar step to the Theorem 2.2.2, multiplying a constant $\lambda > 0$ and exponenting both side.
  \begin{align*}
    P\left( \sum_{i=1}^N X_i \ge t \right) 
    &= P \left( \lambda \sum_{i=1}^N X_i \ge \lambda t\right)
    \\
    &= P \left( \exp\left(\lambda \sum_{i=1}^N X_i\right) \ge \exp(\lambda t) \right).
  \end{align*}
  By the Markov inequality, the probability we consider can be bounded, where we used the independency of the r.v.s $X_i$:
  \[
    P \left( \prod_{i=1}^N \exp(\lambda X_i) \ge \exp(\lambda t) \right) \le e^{-\lambda t} \prod_{i=1}^N \E \exp(\lambda X_i).
  \]
  Now, we need to prove similar version of Exercise 2.2.3, that
  \[
    \E \exp(\lambda X_i) \le \exp\left( \frac{\lambda^2 (M_i - m_i)^2}{8} \right).
  \]
  We first note that, every $X_i$ is the interpolator of $(m_i, M_i)$. 
  We will write it by $X_i = Y_i m_i + (1 - Y_i) M_i$ with $Y_i \in [0, 1]$.
  Since the $\exp(\lambda x)$ is the convex function, we can apply the Jensen's inequality, giving
  \[
    \exp(\lambda X_i) = \exp(\lambda (Y_i m_i + (1 - Y_i) M_i)) \le Y_i \exp(\lambda m_i) + (1 - Y_i) \exp(\lambda M_i)
  \]
  Then taking the expectation gives
  \[
    \E \exp(\lambda X_i) \le \frac{M_i}{M_i - m_i} \exp(\lambda m_i) - \frac{m_i}{M_i - m_i} \exp(\lambda M_i)
  \]
  From our required bound, we can motivate to rewrite this as function of $t = \lambda (M_i - m_i)$:
  \begin{align*}
    &\log \left( \frac{M_i}{M_i - m_i} \exp(\lambda m_i) - \frac{m_i}{M_i - m_i} \exp(\lambda M_i) \right)
    \\
    = &\lambda m_i + \log \left( \frac{M_i}{M_i - m_i} - \frac{m_i}{M_i - m_i} \exp(\lambda (M_i - m_i)) \right)
    \\
    = &\frac{m_i t}{M_i - m_i} + \log \left( 1 + \frac{m_i (1 - e^t)}{M_i - m_i} \right) =: g(t)
  \end{align*}
  Now by Taylor's theorem, there is $\epsilon \in (0, t)$ that
  \[g(t) = g(0) + t g'(0) + \frac{t^2}{2} g''(\epsilon).\]
  Then, we have
  \begin{align*}
    g(0) &= 0,
    \\
    g'(0) &= 0,
    \\
    g''(t) &= - \frac{M_i m_i e^t}{(M_i - m_i e^t)^2}.
  \end{align*}
  Since we have $m_i \le 0 \le M_i$, we can apply the AMGM inequality to dividend, giving
  \[
    \frac{(-m_i e^t) (M_i)}{(M_i - m_i e^t)^2} \le \frac{(M_i - m_i e^t)^2}{4 (M_i - m_i e^t)^2} = \frac{1}{4}.
  \]
  So, we can bound 
  \[
    \E \exp(\lambda X_i) \le \exp\left( \frac{\lambda^2 (M_i - m_i)^2}{8} \right)
  \]
  resulting 
  \[
    p \le \exp\left( -\lambda t + \sum_{i=1}^N \frac{\lambda^2 (M_i - m_i)^2}{8} \right)
  \]
  By letting $\lambda = \frac{2 t}{\sum_{i=1}^N (M_i - m_i)^2}$, we have
  \[
    p \le \exp \left( -\frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right).
  \]
\end{proof}

\begin{customExercise}{8}
  Imagine we have an algorithm for solving some decision problem. Suppose that the algorithm makes a decision at random and returns the correct answer with $1/2+\delta$. To improve the performance, we run the algorithm $N$ times and take the majority vote. Show that, for any $\epsilon \in (0, 1)$, the answer is correct with probability $1-\epsilon$, as long as $N \ge (1/2) \delta^{-2} \log (\epsilon^{-1})$.
\end{customExercise}
\begin{proof}
  Let's assume that the algorithm outputs 1 or -1, where the correct answer is 1. 
  Then $\E X = 2\delta$, so the probability of predicting false is
  \begin{align*}
    &P \left( \sum_{i=1}^N X_i \le 0 \right)
    \\
    = &P \left( \sum_{i=1}^N (X_i - \E X_i) \le - N \E X_i \right)
    \\
    \le &\exp\left( - \frac{ N^2 \delta^2}{N} \right)
  \end{align*}
  where we applied the Theorem 2.2.6 in the last inequality.

  For this bound to hold, we need
  \[\exp \left( -2N \delta^2 \right) \le \epsilon\]
  which is satisfied if $N \ge \frac{\delta^{-2}\log \epsilon^{-1}}{2}$.
\end{proof}

\begin{customExercise}{9}
  Suppose that we want to estimate the mean $\mu$ of a random variable $X$ from a sample $X_1, \ldots, X_N$ drawn indepedently from the distribution of $X$.
  We want an $\epsilon$-accurate estimate, one that falls in the interval $(\mu - \epsilon, \mu + \epsilon)$.

  \begin{enumerate}
    \item Show that a sample of size $N = O(\sigma^2/\epsilon^2)$ is sufficient to compute an $\epsilon$-accurate estimate with probability at least $3/4$, where $\sigma^2 - \Var X$.
    \item Show that a sample of $N = O(\log (\delta^{-1}) \sigma^2/\epsilon^2)$ is sufficient to compute an $\epsilon$-accurate estimate with probability at least $1-\delta$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  For the sample mean of size $N$, we have
  \[P\left( \left|\frac{1}{N}\sum_{i=1}^N X_i - \mu \right| > \epsilon \right)\le \frac{\sigma^2}{N \epsilon^2}\]
  So for this sample mean to fall in the interval $(\mu-\epsilon, \mu+\epsilon)$ with probability at least $\ge 3/4$, we need $N$ to be 
  \[
    N \ge \frac{4 \sigma^2}{\epsilon^2}.
  \]
  Now, naive bound will give $N \ge \frac{\delta^{-1} \sigma^2}{\epsilon^2}$.
  Instead, we will use the median of sample means, $M_{n, k}$ which is median of $2k+1$ sample means, each obtained with $n$ samples.
  
  Let $p_n$ be the probability that the sample mean with $n$ samples, is larger than $\mu + \epsilon$.
  Then the probability that the median of the sample means is larger than $\mu + \epsilon$ is
  \begin{align*}
    &P\left( M_{n,k} \ge \mu + \epsilon \right)
    \\ 
    =& P\left( \sum_{i=1}^{2k+1} \mathbb{I}_{\hat{\mu}_i > \mu + \epsilon} \ge k+1\right)
    \\
    =& P\left( \sum_{i=1}^{2k+1} Y_i \ge k+1 \right)
    \\
    = &P\left( \sum_{i=1}^{2k+1} Y_i - (2k+1) p_n \ge k+1 - (2k+1) p_n \right)
    \\
    \le &\exp \left( -\frac{2 (k+1 - (2k+1)p_n)^2}{2k+1} \right)
  \end{align*}
  where $Y_i \sim \mathrm{Bern}(p_n)$.
  
  Doing this symmetrically, we obtain
  \[P(M_{n,k} \notin (\mu - \epsilon, \mu + \epsilon)) \le 2 \exp \left( - \frac{2 (k+1 - (2k+1) p_n)^2}{2k+1} \right).\]
  Let $n \ge \frac{4\sigma^2}{\epsilon^2}$ so that $p_n \le 1/4$, giving
  \[P(M_{n,k} \notin (\mu - \epsilon, \mu + \epsilon)) \le 2 \exp \left( - \frac{2 (k+1 - \frac{2k+1}{4})^2}{2k+1} \right) \le 2 \exp\left( -\frac{k+1}{4} \right).\]
  So, letting $k$ to
  \[ k \ge 4\log(2/\delta) \]
  gives the probability bounded by $\delta$.

  In summary, we require $(8 \log(2/\delta) + 1) \times \frac{4\sigma^2}{\epsilon^2}$ number of samples.
\end{proof}

\begin{customExercise}{10}
  Let $X_1, \ldots, X_N$ be nonnegative independent random variables with continuous distributions. Assume that the densities of $X_i$ are uniformly bounded by 1.
  \begin{enumerate}
    \item Show that the MGF of $X_i$ satisfies the following for all $t > 0$:
    \[\E \exp(-t X_i) \le 1/t.\]
    \item Deduce that, for any $\epsilon > 0$, we have
    \[P \left( \sum_{i=1}^N X_i \le \epsilon N \right) \le (e \epsilon)^N.\]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  We first have
  \[
    \E \exp(-t X_i) = \int_0^\infty p(x) \exp(-t x) dx \le \int_0^\infty \exp(-tx) dx = \frac{1}{t}.
  \]
  Now, we will do similar strategy as Hoeffding's inequality:
  \begin{align*}
    &P \left( \sum_{i=1}^N X_i \le \epsilon N \right)
    \\
    = &P\left( -\frac{\lambda}{\epsilon} \sum_{i=1}^N X_i \ge -\lambda N \right)
    \\
    = &P\left( \exp\left( -\frac{\lambda}{\epsilon} \sum_{i=1}^N X_i \right) \ge \exp(-\lambda N) \right)
    \\
    \le &\exp(\lambda N) \E \exp\left( -\frac{\lambda}{\epsilon} \sum_{i=1}^N X_i \right)
    \\
    = &\exp(\lambda N) \prod_{i=1}^N \E \exp\left( -\frac{\lambda}{\epsilon} X_i \right)
    \\
    \le &\exp(\lambda N) \frac{\epsilon^N}{\lambda^N}
  \end{align*}
  Taking $\lambda = 1$ gives the bound.
\end{proof}

\setcounter{subsection}{3}

\begin{customExercise}{2}
  Modify the proof of Theorem 2.3.1 to obtain the follwoing bound on the lower tail. For any $t < \mu$, we have
  \[
    P(S_N \le t) \le e^{-\mu} \left( \frac{e\mu}{t} \right)^t.
  \]
\end{customExercise}
\begin{proof}
  We will use $\lambda > 0$, but multiply $-\lambda$ to obtain the following bound on the probability
  \begin{align*}
    P(S_N \le t) 
    &= P(-\lambda S_N \ge -\lambda t)
    \\
    &= P(\exp(-\lambda S_N) \ge \exp(-\lambda t))
    \\
    &\le \exp(\lambda t) \prod_{i=1}^N \E \exp(-\lambda X_i).
  \end{align*}
  Then we can compute the MGF, 
  \[
    \E \exp(-\lambda X_i) = e^{-\lambda} p_i + (1-p_i) = (1 + e^{-\lambda} - 1) p_i \le \exp\left( (e^{-\lambda} - 1) p_i \right).
  \]
  Plugging in, we have
  \[
    \prod_{i=1}^N \E \exp(-\lambda X_i) \le \exp \left( (e^{-\lambda} - 1) \sum_{i=1}^N p_i \right) = \exp\left( (e^{-\lambda} - 1)\mu \right).
  \]
  Finally, we obtain the following bound
  \[
    p \le \exp(\lambda t + (e^{-\lambda} - 1)\mu )
  \]
  and we can set $\lambda = \ln (\mu/t)$, giving
  \[
    p \le \exp\left( \log \left( \frac{\mu}{t} \right)^t + (t-\mu) \right) = \left( \frac{\mu}{t} \right)^t e^t e^{-\mu} = e^{-\mu} \left( \frac{e\mu}{t} \right)^t.
  \]
\end{proof}
\begin{customExercise}{3}
  Let $X \sim \mathrm{Pois}(\lambda)$. Show that, for any $t > \lambda$, we have
  \[
    P(X \ge t) \le e^{-\lambda} \left( \frac{e \lambda}{t} \right)^t.
  \]
\end{customExercise}
\begin{proof}
  We repeat the same procedure, until we have 
  \[
    \E (X \ge t) \le e^{-\eta t} \E \exp(\eta X).
  \]
  We can unroll the p.d.f. of Poisson random variable, which gives
  \[\E \exp(\eta X) = \sum_{k=0}^\infty \frac{\lambda^k e^{\eta k}}{k!} = e^{-\lambda} \sum_{k=0}^\infty \frac{(\lambda e^\eta)^k}{k!} = \exp\left( (e^\eta - 1)\lambda \right).\]
  With $\eta = \log (t/\lambda)$ will prove the result.
\end{proof}

\begin{customExercise}{5}
  Show that, in the setting of Theorem 2.3.1, for $\delta \in (0, 1)$ we have
  \[P \left( |S_N - \mu| \le \delta \mu \right) \le 2 e^{-c \mu \delta^2}\]
  where $c$ is an absolute constant.
\end{customExercise}
\begin{proof}
  We will prove for the single tale.
  For the upper tail, we have
  \[
    P \left( S_n \ge \mu (1 + \delta) \right) \le e^{-\mu} \left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu} = e^{\delta \mu} (1+\delta)^{-(1+\delta)\mu}.
  \]
  Then, it is enough to show that there exists some constant $c > 0$ that
  \[
    e^{\delta \mu} (1+\delta)^{-(1+\delta)\mu} \le e^{-c \mu\delta^2}.
  \]
  Taking the logarithm simplifies to
  \[
    \delta \mu - (1 + \delta) \mu \log(1+\delta) \le -c \mu \delta^2.
  \]
  Now at $\delta = 0$, we have both side zero, so the inequality holds.
  For the inequality to hold, we require the LHS's derivative is smaller than RHS's derivative, 
  \[
    -\mu \log(1+\delta) \le -2c\mu \delta
  \]
  or 
  \[
    \log (1+\delta) \ge 2 c \delta.
  \]
  Since $\log$ is concave function, it is enough to plug in $\delta = 0, 1$, which gives $c \le \frac{\log 2}{2}$.
\end{proof}

\begin{customExercise}{6}
  Let $X \sim \mathrm{Pois}(\lambda)$. Show that for $t \in (0, \lambda)$, we have
  \[
    P (|X-\lambda| \ge t) \le 2 \exp\left( - \frac{2t^2}{\lambda} \right).
  \]
\end{customExercise}
\begin{proof}
  We again prove only for the upper tail.
  By the Exercise 2.3.3, we have
  \[
    P(X \ge t + \lambda) \le e^{-\lambda} \left( \frac{e \lambda}{\lambda + t} \right)^{\lambda + t} = e^{t} \left( \frac{\lambda}{\lambda + t} \right)^{\lambda + t}.
  \]
  Taking the logarithm gives
  \[
    t + (\lambda + t) (\log \lambda - \log (\lambda + t)).
  \]
  which is zero when $t = 0$.
  Again, to show the similar inequality, we should require the derivative w.r.t. $t$,
  \[
    \log \lambda - \log (\lambda + t) \le -\frac{2ct}{\lambda}
  \]
  which is concave function, so it is enough to let
  \[
    \log \left( 1 + \frac{1}{\lambda} \right)^\lambda \ge 1 \ge 2c.
  \]
\end{proof}

\begin{customExercise}{8}
  Let $X \sim \mathrm{Pois}(\lambda)$. Show that, as $\lambda \to \infty$, we have
  \[
    \frac{X - \lambda}{\sqrt{\lambda}} \to \cN(0, 1)
  \]
  in distribution.
\end{customExercise}
\begin{proof}
  Let $N = \lfloor \lambda \rfloor$.
  Then, we can rewrite as
  \[\frac{X - \lambda}{\sqrt{\lambda}} = \frac{\sqrt{N}}{\sqrt{\lambda}} \left( \frac{\sum_{i=1}^N Y_i - N}{\sqrt{N}} + \frac{Z}{\sqrt{N}} - \frac{\lambda - N}{\sqrt{N}} \right)\]
  where $Y_i \sim \mathrm{Pois}(1)$, $Z \sim \mathrm{Pois}(\lambda - N)$.

  Then, we have
  \begin{align*}
    \frac{\sqrt{N}}{\sqrt{\lambda}} &\to 1,
    \\
    \frac{\sum_{i=1}^N Y_i - N}{\sqrt{N}} &\to \cN(0, 1),
    \\
    \frac{Z}{\sqrt{N}} &\to 0,
    \\
    \frac{\lambda - N}{\sqrt{N}} &\to 0,
  \end{align*}
  in distribution.
  Since all other values converge to constant, this limit is simultaneous, and we have the final result.
\end{proof}

\setcounter{subsection}{4}
\begin{customExercise}{2}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = O(\log n)$. Show that with high probability, all the vertices of $G$ have degree $O(\log n)$.
\end{customExercise}
\begin{proof}
  We first have
  \[
    P\left( \exists i. d_i > C \log n \right) \le \sum_{i=1}^n P\left( d_i > C \log n \right) \le n P(d_i > C \log n).
  \]
  so we need to show $P(d_i > C \log n) \le 0.9/n$.
  Now since $d_i \sim B(n-1, p)$, by theorem 2.3.1, and assumption $d \le c \log n$:
  \begin{align*}
    &P\left( d_i > C \log n \right) 
    \\
    \le &e^{-d} \left( \frac{e d}{C \log n} \right)^{C \log n} 
    \\
    = &\exp(C \log n - d) \left( \frac{d}{C \log n} \right)^{C \log n}
    \\
    \le &n^C \exp(-d) \left( c/C \right)^{\log n^C}
    \\
    = &n^C \exp(-d) n^{C \log (c/C)}
    \\
    \le &n^{C (1 + \log (c/C))}. 
  \end{align*}
  The function $C \mapsto C(1 + \log (c/C))$ has derivative $\log (c/C)$, which tends to $-\infty$ as $C$ becomes large, so we can find some $C$ such that
  \[C (1+\log (c/C)) \le -2 \le \log_n 0.9 - 1.\]
  This completes the proof.
\end{proof}

\begin{customExercise}{3}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = O(1)$. Show that with high probability, all the vertices of $G$ has degree 
  \[
    O\left( \frac{\log n}{\log \log n} \right).  
  \]
\end{customExercise}
\begin{proof}
  From the assumption, we have $d \le c$ for some $c$. For simplicity, let's write $t = C \log n / \log \log n$. For simplicity, let's assume that $t$ is integer, and $n \ge e^e$. This is natural assumption, since our bound contains $\log \log n$.
  
  Instead of Chernoff bound, we will use the Union-bound approach.
  We can first give the bound as
  \begin{align*}
    &P\left( d_i > t \right) 
    \\
    = &P\left( \exists S \subset [d] \setminus \{i\}, |S| = t .e_{ij} = 1 : j \in S \right)
    \\
    \le &\sum_{\substack{S \subset [d] \setminus \{i\}\\|S| = t}} P\left( e_{ij = 1} : j \in S \right)
    \\
    = & \binom{n-1}{t } p^{t}.
  \end{align*}
  So we need to show that
  \[
    n \binom{n-1}{t} p^{t} \le 0.9.
  \]
  Using the Exercise 0.0.5, we have
  \[
    n \binom{n-1}{t} p^{t}
    \le n \left( \frac{e(n-1)p}{t} \right)^{t}
    = n \left( \frac{ed }{t} \right)^{t} \le n \left( \frac{ec}{t} \right)^{t}
  \]
  To simplify, let's take logarithm, giving
  \begin{align*}
    &\log n + t \left( \log c + 1 - \log t \right) 
    \\
    = &\log n + t (\log c + 1 - \log C - \log \log n + \log \log \log n)
    \\
    = &\log n - t \log \log n \left( 1 - \frac{\log c + 1 - \log C + \log \log \log n}{\log \log n} \right)
    \\
    = &\log n - C \log n \left( 1 - \frac{\log c + 1 - \log C + \log \log \log n}{\log \log n} \right)
    \\
    = &\log n \left( 1 + \frac{\log c + 1 - \log C + \log \log \log n}{\log \log n} - C \right)
    \\
    \le &\log n \left( \log c + 2 - C \right)
  \end{align*}
  So taking
  \[C \ge \log c + 2 - \frac{0.9}{e}\]
  finishes the proof.
\end{proof}

\begin{customExercise}{4}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = o(\log n)$. Show that, with high probability, $G$ has a vertex with degree $10d$.
\end{customExercise}
\begin{proof}
  Let's take $0 < k < n$. 
  Then, the probability that a graph has some vertex with degree $10d$ is lower bounded as
  \begin{align*}
    &P\left( \exists i. d_i > 10d \right)
    \\
    \ge &k P\left( B(n - k, p) > 10d \right)
    \\
    = &k (1 - P\left( B(n - k, p) \le 10d \right))
  \end{align*}
  by limiting the candidates for large degree to $k$ first vertices, and ignore the edges between them.
  For simplicity, let's take $k = n/2$, so that
  \[
    P\left( \exists i. d_i > 10d \right) \le \frac{n}{2} (1 - P(B(n/2, p) \le 10d))
  \]
  and it is enough to show that
  \[
    P(B(n/2, p) \le 10d) \le 1 - \frac{2C}{n}.
  \]
  Now, using the Exercise 2.3.2, we have
  \[
    P(B(n/2, p) \le 10d) \le e^{-np/2} \left( \frac{enp/2}{10d} \right)^{10d}= \exp\left( 10d - \frac{np}{2} \right) \left( \frac{np}{20d} \right)^{10d}.
  \]
  Since $d = (n-1)p = o(\log n)$, let's take $\epsilon = 1$, that there exists $n_0$ s.t. $n \ge n_0$ implies $d = (n-1)p \le \log n$.
  Let's assume that $n \ge n_0$ now.

  Then, we can rewrite the bound as
  \[
    p \le \exp\left( 10 \log n \right) \left( \frac{np}{20d} \right)^{10 \log n} \le n^{10} = n^{10 - 10 \log 10} 
  \]
  Take $C = 1$, and assume $n_0 >= 2$.
  Then we have
  \[p \le 0.0002\]
  and
  \[
    P(\exists i. d_i > 10d) \ge 0.9998.
  \]
\end{proof}

\begin{customExercise}{5}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = O(1)$.
  Show that, with probability of 0.9, $G$ has a vertex whose degree is at least of order 
  \[
    \frac{\log n}{\log \log n}.
  \]
\end{customExercise}
\begin{proof}
  We will do the similar strategy as 2.4.4.
  For simplicity, write $t = \frac{C \log n}{\log \log n}$.
  Then the probability we are concerned can be lower bounded as
  \begin{align*}
    P(\exists i. d_i > t)
    \ge \frac{n}{2} P\left( B(n/2, p) > t \right)
    = \frac{n}{2} \left( 1 - P\left( B(n/2, p) \le t \right) \right).
  \end{align*}
  So, it is enough to show that
  \[
    P\left( B(n/2, p) \le t \right) \le 1 - \frac{1.8}{n}.
  \]
  By Exercise 2.3.2, we have
  \[
    P\left( B(n/2, p) \le t \right) \le e^{-np/2} \left( \frac{enp/2}{t} \right)^{t}.
  \]
  By assumption, we have some $c$ that $np \le c$, which gives
  \[
    P\left( B(n/2, p) \le t \right) \le e^{-np/2}\left( \frac{ec}{2t} \right)^t \le \left( \frac{ec}{2t} \right)^t.
  \]
  Taking logarithm, we have
  \begin{align*}
    &t (\log c + 1 - \log 2 - \log t)
    \\ = &t (\log c + 1 - \log 2 - \log C  - \log \log n + \log \log \log n)
    \\ =& t\log \log n \left(\frac{\log c + 1 - \log 2 - \log C + \log \log \log n}{\log \log n}-1\right)
    \\ =& C \log n \left(\frac{\log c + 1 - \log 2 - \log C + \log \log \log n}{\log \log n}-1\right)
    \\ \le&C \log n \left( \frac{\log c + 1 + \log \log \log n}{\log \log n} - 1 \right)
  \end{align*}
  For simplicity, let's assume that $n > n_0$ for some $n_0 > 18$, so that
  \[
    \frac{\log c + 1 + \log \log \log n}{\log \log n} - 1 < -\frac{1}{2}.
  \]
  Then, we have
  \[
    p \le n^{-C/2}.
  \]
  and it is enough to show that
  \[
    n^{-C/2} + \frac{1.8}{n} \le 1.
  \]
  Since this is strictly decreasing function on $n$, it is enough to set $C$ small enough to satisfy
  \[
    n_0^{-C/2} \le 0.9.
  \]
\end{proof}

\setcounter{subsection}{5}
\begin{customExercise}{1}
  Show that, for each $p \ge 1$, the random variable $X \sim \cN(0,1)$ satisfies
  \[
    \|X\|_{L^p} = (\E |X|^p)^{1/p} = \sqrt{2} \left( \frac{\Gamma((1+p)/2)}{\Gamma(1/2)} \right)^{1/p}.
  \]
\end{customExercise}
\begin{proof}
  We can first write
  \begin{align*}
    \E |X|^p 
    &= 2\int_0^\infty t^p \frac{1}{\sqrt{2\pi}} \exp(- t^2/2) dt
    \\
    &= \frac{\sqrt{2}}{\pi} \int_0^\infty t^p \exp(- t^2/2) dt.
  \end{align*}
  Let's write the integral as $I_p$.
  
  If we apply integration by parts to $I_{p-2}$, we have
  \begin{align*}
    &\int_0^\infty t^{p-2} \exp(-t^2/2) dt
    \\
    = &\left[ \frac{1}{p-1} t^{p-1} \exp(-t^2/2) \right]_0^\infty + \int_0^\infty \frac{1}{p-1} t^p \exp(-t^2/2) dt
  \end{align*}
  and we obtain the recursive definition
  \[
    I_p = (p-1) I_{p-2}.
  \]
  Unrolling this gives
  \[
    I_p = (p-1)!! \times \begin{cases}
      I_1 & p\text{ is odd} \\
      I_0 & p\text{ is even}
    \end{cases}
  \]
  and we have
  \begin{align*}
    I_1 &= \sqrt{\pi}, \\
    I_0 &= \frac{\pi}{\sqrt{2}}.
  \end{align*}
  This gives
  \[
    \E |X|^p = (p-1)!! \times \begin{cases}
      \sqrt{2/\pi} & p\text{ is odd} \\
      1 & p\text{ is even}
    \end{cases}
  \]
  And rewriting the double factorial to Gamma function gives the result.
\end{proof}

\begin{customExercise}{4}
  Show that the condition $\E X = 0$ is necessary for property (v) to hold.
\end{customExercise}
\begin{proof}
  By Jensen's inequality, we have
  \[
    \exp(\E \lambda X) \le \E \exp(\lambda X) \le \exp(K_t^2 \lambda^2).
  \]
  We can rewrite this as
  \[
    \E X \le K_t^2 \lambda
  \]
  for arbitrary $\lambda$, which is not satisfied if we set $\lambda$ arbitrary small.
\end{proof}

\begin{customExercise}{5}
  \begin{enumerate}
    \item Show that if $X \sim \cN(0, 1)$, the function $\lambda \mapsto \E \exp(\lambda^2 X^2)$ of $X^2$ is finite only in some bounded neighborhood of zero.
    \item Suppose that some random variable $\E \exp(\lambda^2 X^2) \le \exp(K \lambda^2)$ for all $\lambda \in \R$ and some constant $K$> Show that $X$ is a bounded random variable.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item The given function can be written in integral form:
    \[
      \frac{1}{\sqrt{2\pi}}\int_\R \exp\left( \lambda^2 x^2 - \frac{x^2}{2} \right) dx
    \]
    This integral is finite only when $\lambda^2 - 1/2$ is negative, otherwise infinite.
    \item The following holds for all $\lambda$:
    \begin{align*}
      P\left( |X| \ge t \right) 
      &\le P \left( \exp(\lambda^2 X^2) \le \exp(\lambda^2 t^2)\right)
      \\
      &\le \exp(-\lambda^2 t^2) \E \exp(\lambda^2 X^2)
      \\
      &\le \exp\left( \lambda^2 (K - t^2) \right).
    \end{align*}
    Let $t$ large enough so that $K - t^2 < 0$.
    Then, by taking the infimum for $\lambda$ for both side, we have $P(|X| \ge t) = 0$, and $\|X\|_\infty \le \sqrt{K}$.
  \end{enumerate}
\end{proof}

\begin{customExercise}{7}
  Check that $\|\cdot\|_{\psi_2}$ is indeed a norm on the space of sub-gaussian random variables.
\end{customExercise}
\begin{proof}
  We will use the fact that $\exp(x^2)$ is convex.
  For $a, b > 0$, we have
  \begin{align*}
    \E \exp\left( \frac{|X + Y|}{a + b} \right)^2 
    &\le \E \exp\left( \frac{|X| + |Y|}{a + b} \right)^2 
    \\
    &\le \frac{a}{a+b} \E \exp\left( \frac{|X|}{a} \right)^2 + \frac{b}{a+b} \E \exp\left( \frac{|Y|}{b} \right)^2
  \end{align*}
  Now if we plug in $a = \|X\|_{\psi_2}$ and $b = \|Y\|_{\psi_2}$, we have
  \[
    \E \exp\left( \frac{|X|}{\|X\|_{\psi_2}} \right)^2 \le 2,\quad\quad \E \exp\left( \frac{|Y|}{\|Y\|_{\psi_2}} \right)^2 \le 2
  \]
  which gives 
  \[
    \E \exp\left( \frac{|X + Y|}{a + b} \right)^2 \le \frac{2a}{a+b} + \frac{2b}{a+b} = 2.
  \]
\end{proof}

\begin{customExercise}{9}
  Check that the Poisson, exponential, Pareto, and Cauchy distributions are not sub-gaussian.
\end{customExercise}
\begin{proof}
  \begin{description}
    \item[Poisson] The Poisson random variable $X \sim \mathrm{Poisson}(\lambda)$ satisfy
    \[
      (\E |X|^p)^{1/p} = \lambda \exp\left( \frac{p}{2\lambda} \right) \gtrapprox \sqrt{p}.
    \]
    \item[Exponential] The exponential random variable $X \sim \mathrm{Exp}(\lambda)$ satisfy
    \[
      (\E |X|^p)^{1/p} = \frac{(p!)^{1/p}}{\lambda} \ge \frac{p}{e\lambda} \gtrapprox \sqrt{p}.
    \]
    \item[Pareto] The Pareto random variable $X \sim \mathrm{Pareto}(0, \alpha)$ satisfy
    \[
      \E |X|^p = \infty \text{ for } p \ge \alpha.
    \]
    \item[Cauchy] The Cauchy random variable $X \sim \mathrm{Cauchy}(0, \gamma)$ satisfy
    \[
      \E |X|^p = \infty \text{ for } p \ge 1.
    \]
  \end{description}
\end{proof}

\begin{customExercise}{10}
  Let $X_1, X_2, \ldots$ be an infinite sequence of sub-gaussian random variables which are not necessarily independent. 
  Show that
  \[
    \E \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \le CK,
  \]
  where $K = \max_i \|X_i\|_{\psi_2}$.
  Deduce that for every $N \ge 2$ we have
  \[
    \E \max_{i \le N} |X_i| \le CK \sqrt{\log N}.
  \]
\end{customExercise}
\begin{proof}
  We will use the integral form of the expectation since RV is nonnegative, 
  \begin{align*}
    &\E \max_i \frac{|X_i|}{\sqrt{1 + \log i}}
    \\
    = &\int_0^\infty P\left( \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt.
  \end{align*}
  Now, instead of this integral, let's analyze the truncated integral
  \begin{align*}
    &\int_C^\infty P\left( \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt
    \\
    = &\int_C^\infty P\left( \exists i.\  \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt
    \\
    \le &\int_C^\infty \sum_{i=1}^\infty P\left(\frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt
    \\
    = &\int_C^\infty \sum_{i=1}^\infty P\left(|X_i| \ge t\sqrt{1 + \log i} \right) dt
    \\
    \le &\int_C^\infty \sum_{i=1}^\infty 2\exp\left( - C_1 \frac{t^2 (1 + \log i)}{\|X_i\|_{\psi_2}^2} \right) dt
    \\
    = &\int_C^\infty \sum_{i=1}^\infty 2\exp\left( - C_1 \frac{t^2 (1 + \log i)}{K^2} \right) dt
    \\
    \le &2\sum_{i=1}^\infty \exp\left(- \frac{C_1 C^2 \log i}{K^2}\right)\int_C^\infty \exp\left( - C_1 \frac{t^2}{K^2} \right)  dt
    \\
    \le &\frac{K\sqrt{\pi}}{\sqrt{C_1}} \sum_{i=1}^\infty i^{- C_1 C^2 / K^2}
  \end{align*}
  So, if $C$ is large enough, the infinite sum will converge to some finite value, giving this integral have $O(K)$.
  
  Now the rest of integral can be bounded, if we take $C = O(K)$, which gives
  \[
  \int_0^C P\left( \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt \le \int_0^C 1 dt = O(K).
  \]
  
  Finally, let's take $C$ to be $\frac{2K}{\sqrt{C_1}}$, then 
  \begin{align*}
    &\E \max_i \frac{|X_i|}{ \sqrt{1 + \log i}}
    \\
    \le &\frac{2K}{\sqrt{C_1}} + \frac{K\sqrt{\pi}}{\sqrt{C_1}} \sum_{i=1}^\infty i^{-4} 
    \\
    \le &K \left( \frac{180 + \pi^{9/2}}{90\sqrt{C_1}} \right)
  \end{align*}

  For the second part, we can simply set all the $X_i = 0$ for $i > N$, which gives
  \[
    \E \max_{i \le N} |X_i| \le \sqrt{1 + \log N} \E \max_{i} \frac{|X_i|}{\sqrt{1 + \log i}} \le \sqrt{1 + \log N} CK \le 3CK.
  \]
\end{proof}

\begin{customExercise}{11}
  Show that the bound in Exercise 2.5.10 is sharp. 
  Let $X_1, X_2, \ldots, X_N$ be independent $\cN(0, 1)$ random variables. 
  Prove that
  \[
    \E \max_{i \le N} X_i \ge c \sqrt{\log N}.
  \]
\end{customExercise}
\begin{proof}
  We will first split this expectation to two events, 
  \[
    \E \max_{i \le N} X_i = P(X_1 < 0) \E \left[\max_{i \le N} X_i \mid X_1 < 0\right] + P(X_1 \ge 0) \E \left[\max_{i \le N} X_i \mid X_1 \ge 0\right]
  \]
  For the first term, we can lower bound it as
  \begin{align*}
    &P(X_1 < 0) \E \left[\max_{i \le N} X_i \mid X_1 < 0\right] 
    \\
    \ge &\frac{1}{2} \left[ -|X_i| \mid X_1 < 0 \right]
    \\
    = & -\frac{1}{\sqrt{2\pi}}.
  \end{align*}
  For the second term, now it is expectation of nonnegative random variable, we can rewrite it to integral form
  \[
    \E \left[\max_{i \le N} X_i \mid X_1 \ge 0\right] = \int_0^\infty P\left( \max_{i \le N} X_i \ge t \mid X_1 \ge 0\right) dt.
  \]
  Now, note that 
  \begin{align*}
    P(X_1 \ge t \mid X_1 \ge 0) &\ge \frac{2}{\sqrt{2\pi}} \int_0^1 \exp\left( - \frac{(x+y)^2}{2} \right) dy \ge C_1 \exp(-x^2),
    \\
    P(X_i \ge t) &\ge \frac{1}{\sqrt{2\pi}} \int_0^1 \exp\left( - \frac{(x+y)^2}{2} \right) dy \ge C_2 \exp(-x^2).
  \end{align*}
  To use these inequalities, we have
  \begin{align*}
    &\E \left[\max_{i \le N} X_i \mid X_1 \ge 0\right] 
    \\
    = &\int_0^\infty P\left( \max_{i \le N} X_i \ge t \mid X_1 \ge 0\right) dt
    \\
    = &\int_0^\infty 1 - (1 - P(X_1 \ge t \mid X_1 \ge 0))\prod_{i=2}^N\left( 1 - P(X_i \ge t) \right) dt
    \\
    = &\int_0^\infty 1 - (1 - c e^{-t^2})^N dt
  \end{align*}
  If we apply change of variable $x = \sqrt{\log N} u$, we have
  \begin{align*}
    &\int_0^\infty 1 - (1 - c e^{-t^2})^N dt 
    \\
    = &\sqrt{\log N} \int_0^\infty 1 - \left(1 - \frac{c}{N^{u^2}}\right)^N du
    \\
    \ge &\sqrt{\log N} \int_0^\infty 1 - \alpha^{1 / u^2} du
  \end{align*}
  where $\alpha$ does not depend on $N$.

  The last integral is finite for $\alpha < 1$.
  TODO.
\end{proof}

\setcounter{subsection}{6}

\begin{customExercise}{4}
  Deduce Hoeffding's inequality for bounded random variables from Theorem 2.6.3, possibly with some other absolute constant instead of 2 in the exponent.

  \[
    P\left( \left|\sum_{i=1}^N (X_i - \E X_i)\right| \ge t \right) \le 2\exp\left( -\frac{Ct^2}{\sum_{i=1}^N (M_i - m_i)^2} \right).
  \]
\end{customExercise}
\begin{proof}
  By (iii) of Example 2.5.8, each random variables are subgaussian with norms
  \[
    \|X_i - \E X_i\|_{\psi_2} \le C \|X_i - \E X_i\|_\infty = C (M_i - m_i).
  \]
  Then by theorem 2.6.2, we have
  \begin{align*}
    P \left( \left|\sum_{i=1}^N (X_i - \E X_i)\right| \ge t \right) \le 2 \exp\left( - \frac{ct^2}{C^2\sum_{i=1}^N (M_i - m_i)^2} \right).
  \end{align*}
\end{proof}

\begin{customExercise}{5}
  Let $X_1, \ldots, X_N$ be independent sub-gaussian random variables with zero means and unit variances, and let $a = (a_1, \ldots, a_N) \in \R^N$.
  Prove that for every $p \in [2, \infty)$ we have
  \[
    \left( \sum_{i=1}^N a_i^2 \right)^{1/2} \le \left\| \sum_{i=1}^N a_i X_i \right\|_{L^p} \le C K \sqrt{p} \left( \sum_{i=1}^N a_i^2 \right)^{1/2}
  \]
  where $K = \max_i \|X_i\|_{\psi_2}$ and $C$ is an absolute constant.
\end{customExercise}
\begin{proof}
  For the upper bound, we have
  \begin{align*}
    \left\| \sum_{i=1}^N a_i X_i \right\|_{L^p} 
    &= \E \left[ \left| \sum_{i=1}^N a_i X_i \right|^p \right]^{1/p}
    \\
    &= \left( \int_0^\infty P\left( \left| \sum_{i=1}^N a_i X_i \right|^p \ge t \right) dt \right)^{1/p}
    \\
    &= \left( \int_0^\infty P\left( \left| \sum_{i=1}^N a_i X_i \right| \ge t^{1/p} \right) dt \right)^{1/p}
    \\
    &\le \left( \int_0^\infty 2 \exp \left( - \frac{C t^{2/p}}{K^2 \|a\|_2^2}  \right) dt \right)^{1/p}
    \\
    &= \left( 2 \frac{\Gamma(1 + p/2)}{(c/K^2 \|a\|_2^2)^{p/2}} \right)^{1/p}
    \\
    &\le \frac{\sqrt{2}}{\sqrt{c}} K \|a\| \Gamma\left( 1 + \frac{p}{2} \right)^{1/p} 
    \\
    &\le \frac{\sqrt{2}}{\sqrt{c}} K \|a\| \sqrt{p}.
  \end{align*}
  The lower bound is immediate from the Jensen's inequality, as
  \[
    \E \left[ \left| \sum_{i=1}^N a_i X_i \right|^p \right]^{1/p} \ge \E \left[ \left| \sum_{i=1}^N a_i X_i \right|^2 \right]^{1/p \cdot p/2} = \left( \sum_{i=1}^N a_i^2 \right)^{1/2}.
  \]
\end{proof}

\begin{customExercise}{6}
  Show that, in the setting of Exercise 2.6.5, we have
  \[
    c(K) \left( \sum_{i=1}^N a_i^2 \right)^{1/2} \le \left\| \sum_{i=1}^N a_i X_i \right\|_{L^1} \le \left( \sum_{i=1}^N a_i^2 \right)^{1/2}.
  \]
\end{customExercise}
\begin{proof}
  The upper bound is immediate from the Jensen's inequality, as
  \[
    \E \left| \sum_{i=1}^N a_i X_i \right| \le \left( \E \left( \sum_{i=1}^N a_i X_i \right)^2 \right)^{1/2} = \left( \sum_{i=1}^N a_i^2 \right)^{1/2}.
  \]
  For the lower bound, we can use Littlewood's inequality giving
  \[
    \left\|\sum_{i=1}^N a_i X_i \right\|_2 \le \left\|\sum_{i=1}^N a_i X_i \right\|_1^{1/4 } \left\|\sum_{i=1}^N a_i X_i \right\|_3^{3/4}. 
  \]
  By previous theorem, we have
  \[
    \left\|\sum_{i=1}^N a_i X_i \right\|_3^{3/4} \le (CK)^{3/4} p^{3/8} \left( \sum_{i=1}^N a_i^2 \right)^{3/8}
  \]
  This gives
  \[
    \left(\sum_{i=1}^N a_i^2\right)^{1/2} \le \left\|\sum_{i=1}^N a_i X_i \right\| _1^{1/4 }(\sqrt{3} CK)^{3/4}  \left( \sum_{i=1}^N a_i^2 \right)^{3/8}
  \]
  and
  \[
    (\sqrt{3} CK)^{-3} \left( \sum_{i=1}^N a_i^2 \right)^{1/2} \le \left\| \sum_{i=1}^N a_i X_i \right\|_1.
  \]
\end{proof}

\begin{customExercise}{7}
  State and prove a version of Khintchine's inequality for $p \in (0, 2)$.
\end{customExercise}
\begin{proof}
  The upper bound comes from Jensen's inequality, as
  \[
    \left(\E \left| \sum_{i=1}^N a_i X_i \right|^p\right)^{1/p} \le \left( \E \left( \sum_{i=1}^N a_i X_i \right)^2 \right)^{p/2 \cdot 1/p} = \left( \sum_{i=1}^N a_i^2 \right)^{1/2}.
  \]
  For the lower bound, we can use Little's inequality giving
  \[
    \left\| \sum_{i=1}^N a_i X_i \right\|_2 \le \left\| \sum_{i=1}^N a_i X_i \right\|_p^\theta \left\| \sum_{i=1}^N a_i X_i \right\|_{4-p}^{1-\theta} 
  \]
  where $\theta$ satisfies 
  \[
    \frac{1}{2} = \frac{\theta}{p} + \frac{1- \theta}{4-p}.
  \]
  This is done by setting $\theta = p/4$.

  Note that $4 - p \ge 2$, so we can apply Exercise 2.6.5 to have
  \[
    \left\| \sum_{i=1}^N a_i X_i \right\|_{4-p} \le 2CK \left( \sum_{i=1}^N a_i^2 \right)^{1/2}.
  \]
  Then we have
  \[
    \frac{\left(\sum_{i=1}^N a_i^2\right)^{1/2}}{(2CK)^{4/p - 1}} \le \left\| \sum_{i=1}^N a_i X_i \right\|_p.
  \]
  So, the statement is
  \[
    \frac{1}{(2CK)^{4/p - 1}}\left( \sum_{i=1}^N a_i^2 \right)^{1/2} \le \left\| \sum_{i=1}^N a_i X_i \right\|_p \le \left( \sum_{i=1}^N a_i^2 \right)^{1/2}.
  \]
  
\end{proof}

\begin{customExercise}{9}
  Show that, unlike L2-norm, the centering inequality in Lemma 2.6.8 does not hold with $C = 1$.
\end{customExercise}
\begin{proof}
  Consider the random variable 
  \[
    X = \begin{cases}
      1 & \text{with probability }p
      \\
      -1 & \text{with probability }1-p
    \end{cases}
  \]
  which satisfies
  \[
    \E \exp(X^2/t^2) = \exp(1/t^2)
  \]
  and $\|X\|_{\psi_2} = \frac{1}{\sqrt{\log 2}} \approx 1.2011$.

  Now the centered random variable $X - 2p + 1$ satisfies
  \[
    \E \exp(X^2/t^2) = p \exp\left( \frac{(2-2p)^2}{t^2} \right) + (1 - p) \exp\left( \frac{(2p)^2}{t^2} \right)
  \]
  Let $p = 0.25$, then the smallest such $t$ is 
  \[
    \frac{0.5}{\sqrt{0.166078}} \approx 1.22691
  \]
  which satisfies this.
\end{proof}

\setcounter{subsection}{7}

\begin{customExercise}{2}
  Prove the equivalence of properties (i)-(iv) in Proposition 2.7.1 by modifying the proof of Proposition 2.5.2.  
\end{customExercise}
\begin{proof}
  $(i) \Rightarrow (ii)$
  Assume that property (i) holds. By homogenity, we can rescale $X$ to have $K_1 = 1$.
  We can rewrite the moment with integral identity to obtain
  \begin{align*}
    \E |X|^p 
    &= \int_0^\infty P\left( |X| \ge t \right) p t^{p-1} dt
    \\
    &\le 2p \int_0^\infty t^{p-1} \exp\left(- t\right) dt
    \\
    &= 2 p \Gamma(p)
    \\
    &\le 2 p p^p
  \end{align*}
  and taking $p$-th root gives
  \[
    \|X\|_{L^p} \le 2^{1/p} p^{1/p} p \le e^{2/e}p.
  \]
  
  $(ii) \Rightarrow (iii)$
  Assume that (ii) holds, and by rescaling we have $K_2 = 1$.
  By the Taylor series and applying the Monotone convergence theorem, we have
  \[
    \E \exp(\lambda |X|) = \E \left( 1 + \sum_{p=1}^\infty \frac{(\lambda |X|)^p}{p!} \right) = 1 + \sum_{p=1}^\infty \frac{\lambda^p \E |X|^p}{p!}.
  \]
  By assumption we have $\E |X|^p \le p^p$ and the Stirling's approximation gives $p! \ge (p/e)^p$, we have
  \[
    \E \exp(\lambda |X|) \le 1 + \sum_{p=1}^\infty \frac{(\lambda p)^p}{(p/e)^p} = \sum_{p=0}^\infty (\lambda e)^p = \frac{1}{1- \lambda e}
  \]
  when $e \lambda < 1$.
  Finally if $e\lambda < 1/2$, we have the numerical inequality $1 / (1-x) \le e^{2x}$:
  \[
    \E \exp(\lambda |X|) \le \exp(2 \lambda e)
  \]
  for all $\lambda \le 1/2e$.

  $(iii) \Rightarrow (iv)$
  This is trivial, since it is enough to set $K_4$ so that 
  \[
    K_4 \ge K_3 \text{ and }K_4 \ge \frac{K_3}{\log 2}.
  \]

  $(iv) \Rightarrow (i)$
  Again we can assume that $K_4 = 1$. Then we have
  \begin{align*}
    P(|X| \ge t) 
    &= P\left( e^{|X|} \ge e^{t} \right)
    \\
    &\le e^{-t} \E e^{|X|}
    \\
    &\le 2e^{-t}.
  \end{align*}
\end{proof}

\begin{customExercise}{3}
  More generally, consider the class of distributions whose tail decay is of the type $\exp(-ct^\alpha)$ or faster.
  Here $\alpha = 2$ corresponds to sub-gaussian distributions and $\alpha = 1$ to sub-exponential random distributions. 
  State and prove a version of Proposition 2.7.1 for such distributions.
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item $P(|X| \ge t) \le \exp\left( - \frac{t^\alpha}{K_1} \right)$.
    \item $\E |X|^p \le K_2 p^{1/\alpha}$.
    \item $\E \exp(\lambda^\alpha |X|^\alpha) \le \exp(K_3^\alpha \lambda^\alpha)$ for all $\lambda$ such that $|\lambda| \le \frac{1}{K_3}$.
    \item $\E \exp(X^\alpha / K_4^\alpha) \le 2$.
  \end{enumerate}


  $(i) \Rightarrow (ii)$
  Assume that property (i) holds. By homogenity, we can rescale $X$ to have $K_1 = 1$.
  We can rewrite the moment with integral identity to obtain
  \begin{align*}
    \E |X|^p 
    &= \int_0^\infty P\left( |X| \ge t \right) p t^{p-1} dt
    \\
    &\le 2p \int_0^\infty t^{p-1} \exp\left(- t^\alpha\right) dt
    \\
    &= \frac{2p}{\alpha} \int_0^\infty s^{p/\alpha - 1} \exp\left(- s\right) ds
    \\
    &= 2 \frac{p}{\alpha} \Gamma(p/\alpha)
    \\
    &\le 2 p (p/\alpha)^{p/\alpha}
  \end{align*}
  and taking $p$-th root gives
  \[
    \|X\|_{L^p} \le 2^{1/p} p^{1/p} (p / \alpha)^{1/\alpha} \le \frac{1}{\alpha^{1/\alpha}} e^{2/e} p^{1/\alpha}.
  \]
  
  $(ii) \Rightarrow (iii)$
  Assume that (ii) holds, and by rescaling we have $K_2 = 1$.
  By the Taylor series and applying the Monotone convergence theorem, we have
  \[
    \E \exp(\lambda^\alpha |X|^\alpha) = \E \left( 1 + \sum_{p=1}^\infty \frac{(\lambda^\alpha |X|^\alpha)^p}{p!} \right) = 1 + \sum_{p=1}^\infty \frac{\lambda^{\alpha p} \E |X|^{\alpha p}}{p!}.
  \]
  By assumption we have $\E |X|^{\alpha p} \le (\alpha p)^p$ and the Stirling's approximation gives $p! \ge (p/e)^p$, we have
  \[
    \E \exp(\lambda^\alpha |X|^\alpha) \le 1 + \sum_{p=1}^\infty \frac{(\lambda^\alpha \alpha p)^p}{(p/e)^p} = \sum_{p=0}^\infty (\alpha \lambda^\alpha e)^p = \frac{1}{1- \alpha \lambda^\alpha e}
  \]
  when $\alpha \lambda^\alpha e < 1$.
  Finally if $\alpha \lambda^\alpha e < 1/2$, we have the numerical inequality $1 / (1-x) \le e^{2x}$:
  \[
    \E \exp(\lambda^\alpha |X|^\alpha) \le \exp(2 \alpha \lambda^\alpha e)
  \]
  for all $\lambda \le \left(\frac{e}{2\alpha}\right)^{1/\alpha}$.

  $(iii) \Rightarrow (iv)$
  This is trivial, since it is enough to set $K_4$ so that 
  \[
    K_4 \ge K_3 \text{ and }K_4 \ge \frac{K_3}{(\log 2)^{1/\alpha}}.
  \]

  $(iv) \Rightarrow (i)$
  Again we can assume that $K_4 = 1$. Then we have
  \begin{align*}
    P(|X| \ge t) 
    &= P\left( \exp(|X|^\alpha) \ge \exp(t^\alpha) \right)
    \\
    &\le e^{-t^\alpha} \E \exp(|X|^\alpha)
    \\
    &\le 2e^{-t^\alpha}.
  \end{align*}
\end{proof}

\begin{customExercise}{4}
  Argue that the bound in property (iii) in Proposition 2.7.1 cannot be extended to all $\lambda$ such that $|\lambda| \le 1 / K_3$.
\end{customExercise}
\begin{proof}
  Note that $X^2$ for $X \sim \cN(0, 1)$ is sub-exponential but not sub-gaussian, since its moment is exactly square of $X$. 
  Then, we can derive following computation of MGF as
  \begin{align*}
    \E \exp(\lambda |X^2|) 
    &= \frac{1}{\sqrt{2\pi}}\int \exp\left( - \frac{1}{2} x^2 + \lambda x^2 \right) dx 
    \\
    &= \frac{1}{\sqrt{2\pi}} \frac{\sqrt{\pi}}{\sqrt{1/2 - \lambda}}
    \\
    &= \frac{1}{\sqrt{1 - 2\lambda}}
  \end{align*}
  for $\lambda < 1/2$.

  Now if we compare this function and $\exp(a\lambda)$ as the function of $\lambda$, to have the inequality holding at neighborhood of 0, we require the derivative at zero equals which require $a = 1$.
  However, doing so gives 
  \[
    \frac{1}{\sqrt{1 - 2 \lambda}} \ge \exp(\lambda)
  \]
  for $\lambda \in [-1, 1]$, so we can't have desired bound.
\end{proof}

\begin{customExercise}{10}
  Prove an analog of the centering lemma 2.6.8 for subexponential random variables X:
  \[
    \|X - \E X\|_{\psi_1} \le C \|X\|_{\psi_1}.
  \]
\end{customExercise}
\begin{proof}
  We will first prove the similar result as Exercise 2.5.7, that
  \[
    \|X + Y\|_{\psi_1} \le \|X\|_{\psi_1} + \|Y\|_{\psi_1}.
  \]
  Using the convexity of $\exp$, we have
  \begin{align*}
    \E \exp \left( \frac{|X + Y|}{a+b} \right) 
    &\le \E \exp \frac{|X| + |Y|}{a+b}
    \\
    &\le \frac{a}{a+b} \E \exp \frac{|X|}{a} + \frac{b}{a+b} \E \exp \frac{|Y|}{b}
  \end{align*}
  and plugging in the $a = \|X\|_{\psi_1}$ and $\|Y\|_{\psi_1}$ gives the result.
  
  Using this, we have
  \[
    \|X - \E X\|_{\psi_1} \le \|X\|_{\psi_1} + \|\E X\|_{\psi_1}
  \]
  and the sub-exponential norm of constant can be bounded as
  \begin{align*}
    &\|\E X\|_{\psi_1} = \frac{|\E X|}{\log 2} 
    \\
    \le &\frac{\E |X|}{\log 2} = \frac{\|X\|_1}{\log 2} 
    \\
    \le &C \|X\|_{\psi_1}
  \end{align*}
  using the moment bound.
\end{proof}

\begin{customExercise}{11}
  Show that $\|X\|_{\psi}$ is indeed a norm on the space $L_{\psi}$.
\end{customExercise}
\begin{proof}
  The subadditivity can be shown using the convexity, as
  \begin{align*}
    \E \psi \left( \frac{|X + Y|}{a+b} \right)
    &\le \E \psi \left( \frac{|X| + |Y|}{a+b} \right)
    \\
    &\le \frac{a}{a+b} \E \psi \left( \frac{|X|}{a}\right) + \frac{b}{a+b} \E \psi \left( \frac{|Y|}{b}\right)
  \end{align*}
  with $a = \|X\|_{\psi}$ and $b = \|Y\|_{\psi}$. 

  For the positive homogenity, 
  \[
    \E \psi \left( \frac{|aX|}{at} \right) \le 1 \Leftrightarrow \E \psi \left( \frac{|X|}{t} \right) \le 1
  \]
  so we have same norm.

  For the positive definiteness, we actually need quotient space.

  Finally the Non-negativity is immediate since we choose $t$ from $t > 0$.
\end{proof}

\setcounter{subsection}{8}

\begin{customExercise}{5}
  Let $X$ be a zero-mean random variable such that $|X| \le K$. Prove the following bound on the MGF of $X$:
  \[
    \E \exp(\lambda X) \le \exp(g(\lambda) \E X^2)\quad \text{ where }g(\lambda) = \frac{\lambda^2/2}{1 - |\lambda| K/3}
  \]
  provided that $|\lambda| < 3/K$.
\end{customExercise}
\begin{proof}
  Here, ignoring the denominator, $g(\lambda)$ is the second order Taylor expansion of $e^\lambda$, so we can compare following two functions
  \[
    e^x \stackrel{?}{\le} 1 + x + \frac{x^2/2}{1 - |x|/3}.
  \]
  Then we can show that this holds if $|x| < 3$.

  Applying this to MGF gives
  \begin{align*}
    \E \exp(\lambda X) 
    &\le \E \left[ 1 + \lambda X + \frac{\lambda^2 X^2 /2}{1 - |\lambda X|/3} \right]
    \\
    &= 1 + \lambda \E X + \E \left[ \frac{\lambda^2 X^2 /2}{1 - |\lambda X|/3} \right]
    \\
    &\le 1 + \lambda \E X + \E \left[ \frac{\lambda^2 X^2 /2}{1 - |\lambda| K/3} \right]
    \\
    &= 1 + g(\lambda) \E X^2
    \\
    &\le \exp(g(\lambda) \E X^2).
  \end{align*}
  where we used the first order Taylor expansion.
\end{proof}

\begin{customExercise}{6}
  Deduce Theorem 2.8.4 from the bound in Exercise 2.8.5. 
\end{customExercise}
\begin{proof}
  We will prove for one side, 
  \[
    P \left( \sum_{i=1}^n X_i \ge t \right) \le \exp \left( - \frac{t^2/2}{\sigma^2 + Kt/3} \right)
  \]
  We multiply $\lambda$ to both side, and exponentiate this to have
  \[
    P \left( \sum_{i=1}^n X_i \ge t \right) = P \left( \prod_{i=1}^n \exp(\lambda X_i) \ge \exp(\lambda t) \right) \le \exp(- \lambda t) \prod_{i=1}^n \E \exp(\lambda X_i)
  \]
  Using the exercise 2.8.5, we have
  \[
    p \le \exp\left(-\lambda t + g(\lambda) \sigma^2\right)
  \]
  and taking $\lambda = \frac{t}{\sigma^2 + K t/3}$ gives
  \[
    p \le \exp\left( - \frac{t^2}{\sigma^2 + K t/3} + \frac{t^2/2}{\sigma^2 + Kt/3} \right) = \exp \left(-\frac{t^2/2}{\sigma^2 + Kt/3} \right).
  \]
\end{proof}

\end{document}