\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,amsthm,tikz,hyperref}
\usepackage{bbm}

\newtheorem{exercise}{Exercise}[subsection]

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\cN}{\mathcal{N}}
\newcommand*{\Var}{\mathrm{Var}}
\newcommand*{\Cov}{\mathrm{Cov}}
\newcommand*{\esssup}{\mathrm{ess}\,\sup}
\newcommand*{\ind}[1]{\mathbbm{1}_{[{#1}]}}
\newcommand*{\defeq}{\stackrel{\mathrm{def}}{=}}

\renewcommand{\theenumi}{(\alph{enumi})}
\makeatletter

\newenvironment{customExercise}[1]
 {\count@\c@exercise
 \global\c@exercise#1 %
   \global\advance\c@exercise\m@ne
   \exercise}
  {\endexercise
  \global\c@exercise\count@}

\title{Concentration of Sums of Independent Random Variables}
\author{Taeyoung Kim}
\date{\today}

\begin{document}
\maketitle

\setcounter{section}{2}
\setcounter{subsection}{1}

\begin{customExercise}{4}
  Let $g \sim \cN(0, 1)$. Show that, for all $t \ge 1$, we have
  \[\E g^2 \ind{g > t} = t \frac{1}{\sqrt{2\pi}} e^{-t^2/2} + P\left( g > t \right) \le \left( t + \frac{1}{t}\right) \frac{1}{\sqrt{2\pi}} e^{-t^2/2}.\]
\end{customExercise}
\begin{proof}
  We use the following equality
  \[
    x^2 e^{-x^2/2} = e^{-x^2/2} - \frac{d}{dx} \left( x e^{-x^2/2} \right)
  \]
  which gives
  \begin{align*}
    &\E g^2 \ind{g > t} 
    \\
    = &\int_t^\infty g^2 e^{-g^2 / 2} dg 
    \\
    = &\int_t^\infty \left(e^{-g^2/2} - \frac{d}{dg} \left( x e^{-x^2/2} \right) \right)dg
    \\
    = &P(g > t) - \left(\lim_{x \to \infty} \left(x e^{-x^2/2}\right) - t e^{-t^2/2}\right)
    \\
    = &P(g > t) + t e^{-t^2/2}.
  \end{align*}
  The inequality directly follows from Proposition 2.1.3.
\end{proof}

\setcounter{subsection}{2}

\begin{customExercise}{3}
  Show that
  \[\cosh(x) = \frac{e^x + e^{-x}}{2} \le \exp(x^2/2)\]
  for all $x \in \R$.  
\end{customExercise}
\begin{proof}
  Take logarithm to both side, and subtract LHS from RHS. This gives
  \[x^2/2 - \log (\cosh(x))\]
  which is zero at $x=0$.
  Now differentiate this, which gives
  \[x - \tanh(x)\]
  so it is enough to show that 
  \[\tanh(x) \begin{cases}
    \le x & x > 0\\
    \ge x & x < 0
  \end{cases}\]
  Showing the first condition shows the second, since $\tanh$ is symmetric at zero.
  Now differentiate $\tanh$, we have $\tanh'(x) = 1 - \tanh^2(x)$.
  Then for $x > 0$
  \[\tanh(x) = \int_0^x (1 - \tanh^2(t)) dt \le \int_0^x 1 dt = x\]
  finishing the proof. 
  
\end{proof}

\begin{customExercise}{7}
  Prove Theorem 2.2.6, possibly with some absolute constant instead of 2 in the tail.
\end{customExercise}
\begin{proof}
  We will first assume that $\E X_i = 0$ for all $i$, since it does not change both sides.

  Let's take similar step to the Theorem 2.2.2, multiplying a constant $\lambda > 0$ and exponenting both side.
  \begin{align*}
    P\left( \sum_{i=1}^N X_i \ge t \right) 
    &= P \left( \lambda \sum_{i=1}^N X_i \ge \lambda t\right)
    \\
    &= P \left( \exp\left(\lambda \sum_{i=1}^N X_i\right) \ge \exp(\lambda t) \right).
  \end{align*}
  By the Markov inequality, the probability we consider can be bounded, where we used the independency of the r.v.s $X_i$:
  \[
    P \left( \prod_{i=1}^N \exp(\lambda X_i) \ge \exp(\lambda t) \right) \le e^{-\lambda t} \prod_{i=1}^N \E \exp(\lambda X_i).
  \]
  Now, we need to prove similar version of Exercise 2.2.3, that
  \[
    \E \exp(\lambda X_i) \le \exp\left( \frac{\lambda^2 (M_i - m_i)^2}{8} \right).
  \]
  We first note that, every $X_i$ is the interpolator of $(m_i, M_i)$. 
  We will write it by $X_i = Y_i m_i + (1 - Y_i) M_i$ with $Y_i \in [0, 1]$.
  Since the $\exp(\lambda x)$ is the convex function, we can apply the Jensen's inequality, giving
  \[
    \exp(\lambda X_i) = \exp(\lambda (Y_i m_i + (1 - Y_i) M_i)) \le Y_i \exp(\lambda m_i) + (1 - Y_i) \exp(\lambda M_i)
  \]
  Then taking the expectation gives
  \[
    \E \exp(\lambda X_i) \le \frac{M_i}{M_i - m_i} \exp(\lambda m_i) - \frac{m_i}{M_i - m_i} \exp(\lambda M_i)
  \]
  From our required bound, we can motivate to rewrite this as function of $t = \lambda (M_i - m_i)$:
  \begin{align*}
    &\log \left( \frac{M_i}{M_i - m_i} \exp(\lambda m_i) - \frac{m_i}{M_i - m_i} \exp(\lambda M_i) \right)
    \\
    = &\lambda m_i + \log \left( \frac{M_i}{M_i - m_i} - \frac{m_i}{M_i - m_i} \exp(\lambda (M_i - m_i)) \right)
    \\
    = &\frac{m_i t}{M_i - m_i} + \log \left( 1 + \frac{m_i (1 - e^t)}{M_i - m_i} \right) =: g(t)
  \end{align*}
  Now by Taylor's theorem, there is $\epsilon \in (0, t)$ that
  \[g(t) = g(0) + t g'(0) + \frac{t^2}{2} g''(\epsilon).\]
  Then, we have
  \begin{align*}
    g(0) &= 0,
    \\
    g'(0) &= 0,
    \\
    g''(t) &= - \frac{M_i m_i e^t}{(M_i - m_i e^t)^2}.
  \end{align*}
  Since we have $m_i \le 0 \le M_i$, we can apply the AMGM inequality to dividend, giving
  \[
    \frac{(-m_i e^t) (M_i)}{(M_i - m_i e^t)^2} \le \frac{(M_i - m_i e^t)^2}{4 (M_i - m_i e^t)^2} = \frac{1}{4}.
  \]
  So, we can bound 
  \[
    \E \exp(\lambda X_i) \le \exp\left( \frac{\lambda^2 (M_i - m_i)^2}{8} \right)
  \]
  resulting 
  \[
    p \le \exp\left( -\lambda t + \sum_{i=1}^N \frac{\lambda^2 (M_i - m_i)^2}{8} \right)
  \]
  By letting $\lambda = \frac{2 t}{\sum_{i=1}^N (M_i - m_i)^2}$, we have
  \[
    p \le \exp \left( -\frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right).
  \]
\end{proof}

\begin{customExercise}{8}
  Imagine we have an algorithm for solving some decision problem. Suppose that the algorithm makes a decision at random and returns the correct answer with $1/2+\delta$. To improve the performance, we run the algorithm $N$ times and take the majority vote. Show that, for any $\epsilon \in (0, 1)$, the answer is correct with probability $1-\epsilon$, as long as $N \ge (1/2) \delta^{-2} \log (\epsilon^{-1})$.
\end{customExercise}
\begin{proof}
  Let's assume that the algorithm outputs 1 or -1, where the correct answer is 1. 
  Then $\E X = 2\delta$, so the probability of predicting false is
  \begin{align*}
    &P \left( \sum_{i=1}^N X_i \le 0 \right)
    \\
    = &P \left( \sum_{i=1}^N (X_i - \E X_i) \le - N \E X_i \right)
    \\
    \le &\exp\left( - \frac{ N^2 \delta^2}{N} \right)
  \end{align*}
  where we applied the Theorem 2.2.6 in the last inequality.

  For this bound to hold, we need
  \[\exp \left( -2N \delta^2 \right) \le \epsilon\]
  which is satisfied if $N \ge \frac{\delta^{-2}\log \epsilon^{-1}}{2}$.
\end{proof}

\begin{customExercise}{9}
  Suppose that we want to estimate the mean $\mu$ of a random variable $X$ from a sample $X_1, \ldots, X_N$ drawn indepedently from the distribution of $X$.
  We want an $\epsilon$-accurate estimate, one that falls in the interval $(\mu - \epsilon, \mu + \epsilon)$.

  \begin{enumerate}
    \item Show that a sample of size $N = O(\sigma^2/\epsilon^2)$ is sufficient to compute an $\epsilon$-accurate estimate with probability at least $3/4$, where $\sigma^2 - \Var X$.
    \item Show that a sample of $N = O(\log (\delta^{-1}) \sigma^2/\epsilon^2)$ is sufficient to compute an $\epsilon$-accurate estimate with probability at least $1-\delta$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  For the sample mean of size $N$, we have
  \[P\left( \left|\frac{1}{N}\sum_{i=1}^N X_i - \mu \right| > \epsilon \right)\le \frac{\sigma^2}{N \epsilon^2}\]
  So for this sample mean to fall in the interval $(\mu-\epsilon, \mu+\epsilon)$ with probability at least $\ge 3/4$, we need $N$ to be 
  \[
    N \ge \frac{4 \sigma^2}{\epsilon^2}.
  \]
  Now, naive bound will give $N \ge \frac{\delta^{-1} \sigma^2}{\epsilon^2}$.
  Instead, we will use the median of sample means, $M_{n, k}$ which is median of $2k+1$ sample means, each obtained with $n$ samples.
  
  Let $p_n$ be the probability that the sample mean with $n$ samples, is larger than $\mu + \epsilon$.
  Then the probability that the median of the sample means is larger than $\mu + \epsilon$ is
  \begin{align*}
    &P\left( M_{n,k} \ge \mu + \epsilon \right)
    \\ 
    =& P\left( \sum_{i=1}^{2k+1} \mathbb{I}_{\hat{\mu}_i > \mu + \epsilon} \ge k+1\right)
    \\
    =& P\left( \sum_{i=1}^{2k+1} Y_i \ge k+1 \right)
    \\
    = &P\left( \sum_{i=1}^{2k+1} Y_i - (2k+1) p_n \ge k+1 - (2k+1) p_n \right)
    \\
    \le &\exp \left( -\frac{2 (k+1 - (2k+1)p_n)^2}{2k+1} \right)
  \end{align*}
  where $Y_i \sim \mathrm{Bern}(p_n)$.
  
  Doing this symmetrically, we obtain
  \[P(M_{n,k} \notin (\mu - \epsilon, \mu + \epsilon)) \le 2 \exp \left( - \frac{2 (k+1 - (2k+1) p_n)^2}{2k+1} \right).\]
  Let $n \ge \frac{4\sigma^2}{\epsilon^2}$ so that $p_n \le 1/4$, giving
  \[P(M_{n,k} \notin (\mu - \epsilon, \mu + \epsilon)) \le 2 \exp \left( - \frac{2 (k+1 - \frac{2k+1}{4})^2}{2k+1} \right) \le 2 \exp\left( -\frac{k+1}{4} \right).\]
  So, letting $k$ to
  \[ k \ge 4\log(2/\delta) \]
  gives the probability bounded by $\delta$.

  In summary, we require $(8 \log(2/\delta) + 1) \times \frac{4\sigma^2}{\epsilon^2}$ number of samples.
\end{proof}

\begin{customExercise}{10}
  Let $X_1, \ldots, X_N$ be nonnegative independent random variables with continuous distributions. Assume that the densities of $X_i$ are uniformly bounded by 1.
  \begin{enumerate}
    \item Show that the MGF of $X_i$ satisfies the following for all $t > 0$:
    \[\E \exp(-t X_i) \le 1/t.\]
    \item Deduce that, for any $\epsilon > 0$, we have
    \[P \left( \sum_{i=1}^N X_i \le \epsilon N \right) \le (e \epsilon)^N.\]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  We first have
  \[
    \E \exp(-t X_i) = \int_0^\infty p(x) \exp(-t x) dx \le \int_0^\infty \exp(-tx) dx = \frac{1}{t}.
  \]
  Now, we will do similar strategy as Hoeffding's inequality:
  \begin{align*}
    &P \left( \sum_{i=1}^N X_i \le \epsilon N \right)
    \\
    = &P\left( -\frac{\lambda}{\epsilon} \sum_{i=1}^N X_i \ge -\lambda N \right)
    \\
    = &P\left( \exp\left( -\frac{\lambda}{\epsilon} \sum_{i=1}^N X_i \right) \ge \exp(-\lambda N) \right)
    \\
    \le &\exp(\lambda N) \E \exp\left( -\frac{\lambda}{\epsilon} \sum_{i=1}^N X_i \right)
    \\
    = &\exp(\lambda N) \prod_{i=1}^N \E \exp\left( -\frac{\lambda}{\epsilon} X_i \right)
    \\
    \le &\exp(\lambda N) \frac{\epsilon^N}{\lambda^N}
  \end{align*}
  Taking $\lambda = 1$ gives the bound.
\end{proof}

\setcounter{subsection}{3}

\begin{customExercise}{2}
  Modify the proof of Theorem 2.3.1 to obtain the follwoing bound on the lower tail. For any $t < \mu$, we have
  \[
    P(S_N \le t) \le e^{-\mu} \left( \frac{e\mu}{t} \right)^t.
  \]
\end{customExercise}
\begin{proof}
  We will use $\lambda > 0$, but multiply $-\lambda$ to obtain the following bound on the probability
  \begin{align*}
    P(S_N \le t) 
    &= P(-\lambda S_N \ge -\lambda t)
    \\
    &= P(\exp(-\lambda S_N) \ge \exp(-\lambda t))
    \\
    &\le \exp(\lambda t) \prod_{i=1}^N \E \exp(-\lambda X_i).
  \end{align*}
  Then we can compute the MGF, 
  \[
    \E \exp(-\lambda X_i) = e^{-\lambda} p_i + (1-p_i) = (1 + e^{-\lambda} - 1) p_i \le \exp\left( (e^{-\lambda} - 1) p_i \right).
  \]
  Plugging in, we have
  \[
    \prod_{i=1}^N \E \exp(-\lambda X_i) \le \exp \left( (e^{-\lambda} - 1) \sum_{i=1}^N p_i \right) = \exp\left( (e^{-\lambda} - 1)\mu \right).
  \]
  Finally, we obtain the following bound
  \[
    p \le \exp(\lambda t + (e^{-\lambda} - 1)\mu )
  \]
  and we can set $\lambda = \ln (\mu/t)$, giving
  \[
    p \le \exp\left( \log \left( \frac{\mu}{t} \right)^t + (t-\mu) \right) = \left( \frac{\mu}{t} \right)^t e^t e^{-\mu} = e^{-\mu} \left( \frac{e\mu}{t} \right)^t.
  \]
\end{proof}
\begin{customExercise}{3}
  Let $X \sim \mathrm{Pois}(\lambda)$. Show that, for any $t > \lambda$, we have
  \[
    P(X \ge t) \le e^{-\lambda} \left( \frac{e \lambda}{t} \right)^t.
  \]
\end{customExercise}
\begin{proof}
  We repeat the same procedure, until we have 
  \[
    \E (X \ge t) \le e^{-\eta t} \E \exp(\eta X).
  \]
  We can unroll the p.d.f. of Poisson random variable, which gives
  \[\E \exp(\eta X) = \sum_{k=0}^\infty \frac{\lambda^k e^{\eta k}}{k!} = e^{-\lambda} \sum_{k=0}^\infty \frac{(\lambda e^\eta)^k}{k!} = \exp\left( (e^\eta - 1)\lambda \right).\]
  With $\eta = \log (t/\lambda)$ will prove the result.
\end{proof}

\begin{customExercise}{5}
  Show that, in the setting of Theorem 2.3.1, for $\delta \in (0, 1)$ we have
  \[P \left( |S_N - \mu| \le \delta \mu \right) \le 2 e^{-c \mu \delta^2}\]
  where $c$ is an absolute constant.
\end{customExercise}
\begin{proof}
  We will prove for the single tale.
  For the upper tail, we have
  \[
    P \left( S_n \ge \mu (1 + \delta) \right) \le e^{-\mu} \left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu} = e^{\delta \mu} (1+\delta)^{-(1+\delta)\mu}.
  \]
  Then, it is enough to show that there exists some constant $c > 0$ that
  \[
    e^{\delta \mu} (1+\delta)^{-(1+\delta)\mu} \le e^{-c \mu\delta^2}.
  \]
  Taking the logarithm simplifies to
  \[
    \delta \mu - (1 + \delta) \mu \log(1+\delta) \le -c \mu \delta^2.
  \]
  Now at $\delta = 0$, we have both side zero, so the inequality holds.
  For the inequality to hold, we require the LHS's derivative is smaller than RHS's derivative, 
  \[
    -\mu \log(1+\delta) \le -2c\mu \delta
  \]
  or 
  \[
    \log (1+\delta) \ge 2 c \delta.
  \]
  Since $\log$ is concave function, it is enough to plug in $\delta = 0, 1$, which gives $c \le \frac{\log 2}{2}$.
\end{proof}

\begin{customExercise}{6}
  Let $X \sim \mathrm{Pois}(\lambda)$. Show that for $t \in (0, \lambda)$, we have
  \[
    P (|X-\lambda| \ge t) \le 2 \exp\left( - \frac{2t^2}{\lambda} \right).
  \]
\end{customExercise}
\begin{proof}
  We again prove only for the upper tail.
  By the Exercise 2.3.3, we have
  \[
    P(X \ge t + \lambda) \le e^{-\lambda} \left( \frac{e \lambda}{\lambda + t} \right)^{\lambda + t} = e^{t} \left( \frac{\lambda}{\lambda + t} \right)^{\lambda + t}.
  \]
  Taking the logarithm gives
  \[
    t + (\lambda + t) (\log \lambda - \log (\lambda + t)).
  \]
  which is zero when $t = 0$.
  Again, to show the similar inequality, we should require the derivative w.r.t. $t$,
  \[
    \log \lambda - \log (\lambda + t) \le -\frac{2ct}{\lambda}
  \]
  which is concave function, so it is enough to let
  \[
    \log \left( 1 + \frac{1}{\lambda} \right)^\lambda \ge 1 \ge 2c.
  \]
\end{proof}

\begin{customExercise}{8}
  Let $X \sim \mathrm{Pois}(\lambda)$. Show that, as $\lambda \to \infty$, we have
  \[
    \frac{X - \lambda}{\sqrt{\lambda}} \to \cN(0, 1)
  \]
  in distribution.
\end{customExercise}
\begin{proof}
  Let $N = \lfloor \lambda \rfloor$.
  Then, we can rewrite as
  \[\frac{X - \lambda}{\sqrt{\lambda}} = \frac{\sqrt{N}}{\sqrt{\lambda}} \left( \frac{\sum_{i=1}^N Y_i - N}{\sqrt{N}} + \frac{Z}{\sqrt{N}} - \frac{\lambda - N}{\sqrt{N}} \right)\]
  where $Y_i \sim \mathrm{Pois}(1)$, $Z \sim \mathrm{Pois}(\lambda - N)$.

  Then, we have
  \begin{align*}
    \frac{\sqrt{N}}{\sqrt{\lambda}} &\to 1,
    \\
    \frac{\sum_{i=1}^N Y_i - N}{\sqrt{N}} &\to \cN(0, 1),
    \\
    \frac{Z}{\sqrt{N}} &\to 0,
    \\
    \frac{\lambda - N}{\sqrt{N}} &\to 0,
  \end{align*}
  in distribution.
  Since all other values converge to constant, this limit is simultaneous, and we have the final result.
\end{proof}

\setcounter{subsection}{4}
\begin{customExercise}{2}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = O(\log n)$. Show that with high probability, all the vertices of $G$ have degree $O(\log n)$.
\end{customExercise}
\begin{proof}
  We first have
  \[
    P\left( \exists i. d_i > C \log n \right) \le \sum_{i=1}^n P\left( d_i > C \log n \right) \le n P(d_i > C \log n).
  \]
  so we need to show $P(d_i > C \log n) \le 0.9/n$.
  Now since $d_i \sim B(n-1, p)$, by theorem 2.3.1, and assumption $d \le c \log n$:
  \begin{align*}
    &P\left( d_i > C \log n \right) 
    \\
    \le &e^{-d} \left( \frac{e d}{C \log n} \right)^{C \log n} 
    \\
    = &\exp(C \log n - d) \left( \frac{d}{C \log n} \right)^{C \log n}
    \\
    \le &n^C \exp(-d) \left( c/C \right)^{\log n^C}
    \\
    = &n^C \exp(-d) n^{C \log (c/C)}
    \\
    \le &n^{C (1 + \log (c/C))}. 
  \end{align*}
  The function $C \mapsto C(1 + \log (c/C))$ has derivative $\log (c/C)$, which tends to $-\infty$ as $C$ becomes large, so we can find some $C$ such that
  \[C (1+\log (c/C)) \le -2 \le \log_n 0.9 - 1.\]
  This completes the proof.
\end{proof}

\begin{customExercise}{3}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = O(1)$. Show that with high probability, all the vertices of $G$ has degree 
  \[
    O\left( \frac{\log n}{\log \log n} \right).  
  \]
\end{customExercise}
\begin{proof}
  From the assumption, we have $d \le c$ for some $c$. For simplicity, let's write $t = C \log n / \log \log n$. For simplicity, let's assume that $t$ is integer, and $n \ge e^e$. This is natural assumption, since our bound contains $\log \log n$.
  
  Instead of Chernoff bound, we will use the Union-bound approach.
  We can first give the bound as
  \begin{align*}
    &P\left( d_i > t \right) 
    \\
    = &P\left( \exists S \subset [d] \setminus \{i\}, |S| = t .e_{ij} = 1 : j \in S \right)
    \\
    \le &\sum_{\substack{S \subset [d] \setminus \{i\}\\|S| = t}} P\left( e_{ij = 1} : j \in S \right)
    \\
    = & \binom{n-1}{t } p^{t}.
  \end{align*}
  So we need to show that
  \[
    n \binom{n-1}{t} p^{t} \le 0.9.
  \]
  Using the Exercise 0.0.5, we have
  \[
    n \binom{n-1}{t} p^{t}
    \le n \left( \frac{e(n-1)p}{t} \right)^{t}
    = n \left( \frac{ed }{t} \right)^{t} \le n \left( \frac{ec}{t} \right)^{t}
  \]
  To simplify, let's take logarithm, giving
  \begin{align*}
    &\log n + t \left( \log c + 1 - \log t \right) 
    \\
    = &\log n + t (\log c + 1 - \log C - \log \log n + \log \log \log n)
    \\
    = &\log n - t \log \log n \left( 1 - \frac{\log c + 1 - \log C + \log \log \log n}{\log \log n} \right)
    \\
    = &\log n - C \log n \left( 1 - \frac{\log c + 1 - \log C + \log \log \log n}{\log \log n} \right)
    \\
    = &\log n \left( 1 + \frac{\log c + 1 - \log C + \log \log \log n}{\log \log n} - C \right)
    \\
    \le &\log n \left( \log c + 2 - C \right)
  \end{align*}
  So taking
  \[C \ge \log c + 2 - \frac{0.9}{e}\]
  finishes the proof.
\end{proof}

\begin{customExercise}{4}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = o(\log n)$. Show that, with high probability, $G$ has a vertex with degree $10d$.
\end{customExercise}
\begin{proof}
  Let's take $0 < k < n$. 
  Then, the probability that a graph has some vertex with degree $10d$ is lower bounded as
  \begin{align*}
    &P\left( \exists i. d_i > 10d \right)
    \\
    \ge &k P\left( B(n - k, p) > 10d \right)
    \\
    = &k (1 - P\left( B(n - k, p) \le 10d \right))
  \end{align*}
  by limiting the candidates for large degree to $k$ first vertices, and ignore the edges between them.
  For simplicity, let's take $k = n/2$, so that
  \[
    P\left( \exists i. d_i > 10d \right) \le \frac{n}{2} (1 - P(B(n/2, p) \le 10d))
  \]
  and it is enough to show that
  \[
    P(B(n/2, p) \le 10d) \le 1 - \frac{2C}{n}.
  \]
  Now, using the Exercise 2.3.2, we have
  \[
    P(B(n/2, p) \le 10d) \le e^{-np/2} \left( \frac{enp/2}{10d} \right)^{10d}= \exp\left( 10d - \frac{np}{2} \right) \left( \frac{np}{20d} \right)^{10d}.
  \]
  Since $d = (n-1)p = o(\log n)$, let's take $\epsilon = 1$, that there exists $n_0$ s.t. $n \ge n_0$ implies $d = (n-1)p \le \log n$.
  Let's assume that $n \ge n_0$ now.

  Then, we can rewrite the bound as
  \[
    p \le \exp\left( 10 \log n \right) \left( \frac{np}{20d} \right)^{10 \log n} \le n^{10} = n^{10 - 10 \log 10} 
  \]
  Take $C = 1$, and assume $n_0 >= 2$.
  Then we have
  \[p \le 0.0002\]
  and
  \[
    P(\exists i. d_i > 10d) \ge 0.9998.
  \]
\end{proof}

\begin{customExercise}{5}
  Consider a random graph $G \sim G(n, p)$ with expected degrees $d = O(1)$.
  Show that, with probability of 0.9, $G$ has a vertex whose degree is at least of order 
  \[
    \frac{\log n}{\log \log n}.
  \]
\end{customExercise}
\begin{proof}
  We will do the similar strategy as 2.4.4.
  For simplicity, write $t = \frac{C \log n}{\log \log n}$.
  Then the probability we are concerned can be lower bounded as
  \begin{align*}
    P(\exists i. d_i > t)
    \ge \frac{n}{2} P\left( B(n/2, p) > t \right)
    = \frac{n}{2} \left( 1 - P\left( B(n/2, p) \le t \right) \right).
  \end{align*}
  So, it is enough to show that
  \[
    P\left( B(n/2, p) \le t \right) \le 1 - \frac{1.8}{n}.
  \]
  By Exercise 2.3.2, we have
  \[
    P\left( B(n/2, p) \le t \right) \le e^{-np/2} \left( \frac{enp/2}{t} \right)^{t}.
  \]
  By assumption, we have some $c$ that $np \le c$, which gives
  \[
    P\left( B(n/2, p) \le t \right) \le e^{-np/2}\left( \frac{ec}{2t} \right)^t \le \left( \frac{ec}{2t} \right)^t.
  \]
  Taking logarithm, we have
  \begin{align*}
    &t (\log c + 1 - \log 2 - \log t)
    \\ = &t (\log c + 1 - \log 2 - \log C  - \log \log n + \log \log \log n)
    \\ =& t\log \log n \left(\frac{\log c + 1 - \log 2 - \log C + \log \log \log n}{\log \log n}-1\right)
    \\ =& C \log n \left(\frac{\log c + 1 - \log 2 - \log C + \log \log \log n}{\log \log n}-1\right)
    \\ \le&C \log n \left( \frac{\log c + 1 + \log \log \log n}{\log \log n} - 1 \right)
  \end{align*}
  For simplicity, let's assume that $n > n_0$ for some $n_0 > 18$, so that
  \[
    \frac{\log c + 1 + \log \log \log n}{\log \log n} - 1 < -\frac{1}{2}.
  \]
  Then, we have
  \[
    p \le n^{-C/2}.
  \]
  and it is enough to show that
  \[
    n^{-C/2} + \frac{1.8}{n} \le 1.
  \]
  Since this is strictly decreasing function on $n$, it is enough to set $C$ small enough to satisfy
  \[
    n_0^{-C/2} \le 0.9.
  \]
\end{proof}

\setcounter{subsection}{5}
\begin{customExercise}{1}
  Show that, for each $p \ge 1$, the random variable $X \sim \cN(0,1)$ satisfies
  \[
    \|X\|_{L^p} = (\E |X|^p)^{1/p} = \sqrt{2} \left( \frac{\Gamma((1+p)/2)}{\Gamma(1/2)} \right)^{1/p}.
  \]
\end{customExercise}
\begin{proof}
  We can first write
  \begin{align*}
    \E |X|^p 
    &= 2\int_0^\infty t^p \frac{1}{\sqrt{2\pi}} \exp(- t^2/2) dt
    \\
    &= \frac{\sqrt{2}}{\pi} \int_0^\infty t^p \exp(- t^2/2) dt.
  \end{align*}
  Let's write the integral as $I_p$.
  
  If we apply integration by parts to $I_{p-2}$, we have
  \begin{align*}
    &\int_0^\infty t^{p-2} \exp(-t^2/2) dt
    \\
    = &\left[ \frac{1}{p-1} t^{p-1} \exp(-t^2/2) \right]_0^\infty + \int_0^\infty \frac{1}{p-1} t^p \exp(-t^2/2) dt
  \end{align*}
  and we obtain the recursive definition
  \[
    I_p = (p-1) I_{p-2}.
  \]
  Unrolling this gives
  \[
    I_p = (p-1)!! \times \begin{cases}
      I_1 & p\text{ is odd} \\
      I_0 & p\text{ is even}
    \end{cases}
  \]
  and we have
  \begin{align*}
    I_1 &= \sqrt{\pi}, \\
    I_0 &= \frac{\pi}{\sqrt{2}}.
  \end{align*}
  This gives
  \[
    \E |X|^p = (p-1)!! \times \begin{cases}
      \sqrt{2/\pi} & p\text{ is odd} \\
      1 & p\text{ is even}
    \end{cases}
  \]
  And rewriting the double factorial to Gamma function gives the result.
\end{proof}

\begin{customExercise}{4}
  Show that the condition $\E X = 0$ is necessary for property (v) to hold.
\end{customExercise}
\begin{proof}
  By Jensen's inequality, we have
  \[
    \exp(\E \lambda X) \le \E \exp(\lambda X) \le \exp(K_t^2 \lambda^2).
  \]
  We can rewrite this as
  \[
    \E X \le K_t^2 \lambda
  \]
  for arbitrary $\lambda$, which is not satisfied if we set $\lambda$ arbitrary small.
\end{proof}

\begin{customExercise}{5}
  \begin{enumerate}
    \item Show that if $X \sim \cN(0, 1)$, the function $\lambda \mapsto \E \exp(\lambda^2 X^2)$ of $X^2$ is finite only in some bounded neighborhood of zero.
    \item Suppose that some random variable $\E \exp(\lambda^2 X^2) \le \exp(K \lambda^2)$ for all $\lambda \in \R$ and some constant $K$> Show that $X$ is a bounded random variable.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item The given function can be written in integral form:
    \[
      \frac{1}{\sqrt{2\pi}}\int_\R \exp\left( \lambda^2 x^2 - \frac{x^2}{2} \right) dx
    \]
    This integral is finite only when $\lambda^2 - 1/2$ is negative, otherwise infinite.
    \item The following holds for all $\lambda$:
    \begin{align*}
      P\left( |X| \ge t \right) 
      &\le P \left( \exp(\lambda^2 X^2) \le \exp(\lambda^2 t^2)\right)
      \\
      &\le \exp(-\lambda^2 t^2) \E \exp(\lambda^2 X^2)
      \\
      &\le \exp\left( \lambda^2 (K - t^2) \right).
    \end{align*}
    Let $t$ large enough so that $K - t^2 < 0$.
    Then, by taking the infimum for $\lambda$ for both side, we have $P(|X| \ge t) = 0$, and $\|X\|_\infty \le \sqrt{K}$.
  \end{enumerate}
\end{proof}

\begin{customExercise}{7}
  Check that $\|\cdot\|_{\psi_2}$ is indeed a norm on the space of sub-gaussian random variables.
\end{customExercise}
\begin{proof}
  We will use the fact that $\exp(x^2)$ is convex.
  For $a, b > 0$, we have
  \begin{align*}
    \E \exp\left( \frac{|X + Y|}{a + b} \right)^2 
    &\le \E \exp\left( \frac{|X| + |Y|}{a + b} \right)^2 
    \\
    &\le \frac{a}{a+b} \E \exp\left( \frac{|X|}{a} \right)^2 + \frac{b}{a+b} \E \exp\left( \frac{|Y|}{b} \right)^2
  \end{align*}
  Now if we plug in $a = \|X\|_{\psi_2}$ and $b = \|Y\|_{\psi_2}$, we have
  \[
    \E \exp\left( \frac{|X|}{\|X\|_{\psi_2}} \right)^2 \le 2,\quad\quad \E \exp\left( \frac{|Y|}{\|Y\|_{\psi_2}} \right)^2 \le 2
  \]
  which gives 
  \[
    \E \exp\left( \frac{|X + Y|}{a + b} \right)^2 \le \frac{2a}{a+b} + \frac{2b}{a+b} = 2.
  \]
\end{proof}

\begin{customExercise}{9}
  Check that the Poisson, exponential, Pareto, and Cauchy distributions are not sub-gaussian.
\end{customExercise}
\begin{proof}
  \begin{description}
    \item[Poisson] The Poisson random variable $X \sim \mathrm{Poisson}(\lambda)$ satisfy
    \[
      (\E |X|^p)^{1/p} = \lambda \exp\left( \frac{p}{2\lambda} \right) \gtrapprox \sqrt{p}.
    \]
    \item[Exponential] The exponential random variable $X \sim \mathrm{Exp}(\lambda)$ satisfy
    \[
      (\E |X|^p)^{1/p} = \frac{(p!)^{1/p}}{\lambda} \ge \frac{p}{e\lambda} \gtrapprox \sqrt{p}.
    \]
    \item[Pareto] The Pareto random variable $X \sim \mathrm{Pareto}(0, \alpha)$ satisfy
    \[
      \E |X|^p = \infty \text{ for } p \ge \alpha.
    \]
    \item[Cauchy] The Cauchy random variable $X \sim \mathrm{Cauchy}(0, \gamma)$ satisfy
    \[
      \E |X|^p = \infty \text{ for } p \ge 1.
    \]
  \end{description}
\end{proof}

\begin{customExercise}{10}
  Let $X_1, X_2, \ldots$ be an infinite sequence of sub-gaussian random variables which are not necessarily independent. 
  Show that
  \[
    \E \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \le CK,
  \]
  where $K = \max_i \|X_i\|_{\psi_2}$.
  Deduce that for every $N \ge 2$ we have
  \[
    \E \max_{i \le N} |X_i| \le CK \sqrt{\log N}.
  \]
\end{customExercise}
\begin{proof}
  We will use the integral form of the expectation since RV is nonnegative, 
  \begin{align*}
    &\E \max_i \frac{|X_i|}{\sqrt{1 + \log i}}
    \\
    = &\int_0^\infty P\left( \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt.
  \end{align*}
  Now, instead of this integral, let's analyze the truncated integral
  \begin{align*}
    &\int_C^\infty P\left( \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt
    \\
    = &\int_C^\infty P\left( \exists i.\  \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt
    \\
    \le &\int_C^\infty \sum_{i=1}^\infty P\left(\frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt
    \\
    = &\int_C^\infty \sum_{i=1}^\infty P\left(|X_i| \ge t\sqrt{1 + \log i} \right) dt
    \\
    \le &\int_C^\infty \sum_{i=1}^\infty 2\exp\left( - C_1 \frac{t^2 (1 + \log i)}{\|X_i\|_{\psi_2}^2} \right) dt
    \\
    = &\int_C^\infty \sum_{i=1}^\infty 2\exp\left( - C_1 \frac{t^2 (1 + \log i)}{K^2} \right) dt
    \\
    \le &2\sum_{i=1}^\infty \exp\left(- \frac{C_1 C^2 \log i}{K^2}\right)\int_C^\infty \exp\left( - C_1 \frac{t^2}{K^2} \right)  dt
    \\
    \le &\frac{K\sqrt{\pi}}{\sqrt{C_1}} \sum_{i=1}^\infty i^{- C_1 C^2 / K^2}
  \end{align*}
  So, if $C$ is large enough, the infinite sum will converge to some finite value, giving this integral have $O(K)$.
  
  Now the rest of integral can be bounded, if we take $C = O(K)$, which gives
  \[
  \int_0^C P\left( \max_i \frac{|X_i|}{\sqrt{1 + \log i}} \ge t \right) dt \le \int_0^C 1 dt = O(K).
  \]
  
  Finally, let's take $C$ to be $\frac{2K}{\sqrt{C_1}}$, then 
  \begin{align*}
    &\E \max_i \frac{|X_i|}{ \sqrt{1 + \log i}}
    \\
    \le &\frac{2K}{\sqrt{C_1}} + \frac{K\sqrt{\pi}}{\sqrt{C_1}} \sum_{i=1}^\infty i^{-4} 
    \\
    \le &K \left( \frac{180 + \pi^{9/2}}{90\sqrt{C_1}} \right)
  \end{align*}

  For the second part, we can simply set all the $X_i = 0$ for $i > N$, which gives
  \[
    \E \max_{i \le N} |X_i| \le \sqrt{1 + \log N} \E \max_{i} \frac{|X_i|}{\sqrt{1 + \log i}} \le \sqrt{1 + \log N} CK \le 3CK.
  \]
\end{proof}

\begin{customExercise}{11}
  Show that the bound in Exercise 2.5.10 is sharp. 
  Let $X_1, X_2, \ldots, X_N$ be independent $\cN(0, 1)$ random variables. 
  Prove that
  \[
    \E \max_{i \le N} X_i \ge c \sqrt{\log N}.
  \]
\end{customExercise}
\begin{proof}
  
\end{proof}

\end{document}