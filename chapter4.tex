\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,amsthm,tikz,hyperref}
\usepackage{bbm}

\newtheorem{exercise}{Exercise}[subsection]

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\cN}{\mathcal{N}}
\newcommand*{\Var}{\mathrm{Var}}
\newcommand*{\Cov}{\mathrm{Cov}}
\newcommand*{\esssup}{\mathrm{ess}\,\sup}
\newcommand*{\ind}[1]{\mathbbm{1}_{[{#1}]}}
\newcommand*{\defeq}{\stackrel{\mathrm{def}}{=}}
\newcommand*{\cP}{\mathcal{P}}
\newcommand*{\cM}{\mathcal{M}}

\renewcommand{\theenumi}{(\alph{enumi})}
\makeatletter

\newenvironment{customExercise}[1]
 {\count@\c@exercise
 \global\c@exercise#1 %
   \global\advance\c@exercise\m@ne
   \exercise}
  {\endexercise
  \global\c@exercise\count@}

\title{Random Matrices}
\author{Taeyoung Kim}
\date{\today}

\begin{document}
\maketitle

\setcounter{section}{4}
\setcounter{subsection}{1}

\begin{customExercise}{1}
  Suppose that $A$ is an invertible matrix with singular value decomposition
  \[
    A = \sum_{i=1}^n s_i u_i v_i^\intercal.
  \]
  Check that
  \[
    A^{-1} \sum_{i=1}^n \frac{1}{s_i} v_i u_i^\intercal.
  \]
\end{customExercise}
\begin{proof}
  \begin{align*}
    A A^{-1} &= \sum_{i=1}^n s_i u_i v_i^\intercal \sum_{j=1}^n \frac{1}{s_j} v_j u_j^\intercal
    \\
    &= \sum_{i,j=1}^n \frac{s_i}{s_j} u_i v_i^\intercal v_j u_j^\intercal
    \\
    &= \sum_{i=1}^n u_i u_i^\intercal
    \\
    &= I_n.
  \end{align*}
  The case for $A^{-1} A$ is identical.
\end{proof}

\begin{customExercise}{2}
  Prove the following bound on the singular values $s_i$ of any matrix $A$:
  \[
    s_i \le \frac{1}{\sqrt{i}} \|A\|_F.
  \]
\end{customExercise}
\begin{proof}
  \begin{align*}
    i s_i(A)^2 \le \sum_{k=1}^i s_k(A)^2 \le \sum_{k=1}^r s_i(A)^2 \le \|A\|_F^2.
  \end{align*}
\end{proof}

\begin{customExercise}{3}
  Let $A_k$ be the best rank-$k$ approximation of a matrix $A$. 
  Express $\|A - A_K\|^2$ and $\|A - A_k\|_F^2$ in terms of the singular values of $s_i$ of $A$.
\end{customExercise}
\begin{proof}
  \begin{align*}
    \|A - A_k\|^2 &= \left\|\sum_{i=k+1}^r s_i u_i v_i^\intercal \right\|^2 = s_{k+1}.
    \\
    \|A - A_k\|_F^2 &= \left\|\sum_{i=k+1}^r s_i u_i v_i^\intercal\right\|_F^2 = \sum_{i=k+1}^r s_i(A)^2.
  \end{align*}
\end{proof}

\begin{customExercise}{4}
  Let $A$ be an $m \times n$ matrix with $m \ge n$.
  Prove that the following statements are equivalent.
  \begin{enumerate}
    \item $A^\intercal A = I_n$.
    \item $P \defeq AA^\intercal$ is an orthogonal projection in $\R^m$ onto a subspace of dimension $n$.
    \item $A$ is an isometry, or isometrical embedding of $\R^n$ into $\R^m$ which means that $\|A x\|_2 = \|x\|_2$.
    \item All singular values of $A$ equal 1: equivalently
    \[
      s_n(A) = s_1(A) = 1.
    \]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  For all cases, we assume that
  \[
    A = \sum_{i=1}^n s_i u_i v_i^\intercal
  \]
  as the SVD.

  $(a) \Leftrightarrow (d)$.
  \begin{align*}
    A^\intercal A = \sum_{i=1}^n s_i^2 u_i u_i^\intercal
  \end{align*}
  is equal to $I_n$ if and only if $s_i^2 = 1$.

  $(d) \Rightarrow (b)$
  First, 
  \begin{align*}
    P^2 = A A^\intercal A A^\intercal = A A^\intercal = P
  \end{align*}
  by equivalence to $(a)$.
  We then have
  \[
    P = \sum_{i=1}^n s_i u_i v_i^\intercal \sum_{j=1} s_j v_j u_j^\intercal = \sum_{i=1}^n s_i^2 u_i u_i^\intercal.
  \]
  If we set $x = \sum_{i=1}^n b_i u_i + v$ where $v$ is orthogonal to $u_i$, then
  \[
    Px = \sum_{i=1}^n b_i u_i
  \]
  where for $x$ to be zero, it need to have $b_i = 0$, hence orthogonal to all $u_i$. This shows that $P$ is projection.
  And since $u_i$ are the orthonormal basis, the image is subspace of dimension $n$.

  $(b) \Rightarrow (d)$
  \begin{align*}
    P^2 &= A A^\intercal A A^\intercal
    \\
    &= \sum_{i,j,k,l=1}^n s_i s_j s_k s_l u_i v_i^\intercal v_j u_j^\intercal u_k v_k^\intercal v_l u_l^\intercal
    \\
    &= \sum_{i=1}^n s_i^4 u_i u_i^\intercal
  \end{align*}
  and $P = \sum_{i=1}^n s_i^2 u_i u_i^\intercal$.
  For these to be equal, we need $s_i = 1$ or $s_i = 0$.
  However if any of $s_i = 0$, we can't have its image dimension $n$, so this proves $(d)$.

  $(c) \Rightarrow (d)$
  Let $x = v_i$, then
  \[
    1 = \|v_i\| = \|Av_i\| = \left\|\sum_{i=1}^n s_i u_i v_i^\intercal v_i\right\| = \|s_i u_i\| = s_i.
  \]

  $(d) \Rightarrow (c)$
  Let's write $x = \sum_{i=1}^n b_i v_i$, where we have $\sum_{i=1}^n b_i^2 = 1$.
  Then 
  \[
    Ax = \sum_{i=1}^n s_i u_i v_i^\intercal \sum_{j=1}^n b_i v_i = \sum_{i=1}^n b_i s_i u_i = \sum_{i=1}^n b_i u_i
  \]
  which gives $\|Ax\| = \sqrt{\sum_{i=1}^n b_i^2} = 1$.
\end{proof}

\begin{customExercise}{6}
  Prove the following converse to Lemma 4.1.5:
  if 
  \[
    1 - \delta \le s_n(A) \le s_1(A) \le 1 + \delta
  \]
  then
  \[
    \|A^\intercal A - I_n\| \le 3 \max(\delta, \delta^2).
  \]
\end{customExercise}
\begin{proof}
  From the SVD of $A$, we can write
  \[
    A^\intercal A - I_n = \sum_{i=1}^n (s_i(A)^2 - 1) u_i u_i^\intercal.
  \]
  For any vector $x = \sum_{i=1}^n b_i u_i$,
  \begin{align*}
    &\langle (A^\intercal A - I)x, x \rangle
    \\
    = &\left\langle \sum_{i=1}^n b_i (s_i^2 - 1) u_i, \sum_{i=1}^n b_i u_i \right\rangle
    \\
    = &\sum_{i=1}^n b_i^2 (s_i^2 - 1)
    \\
    \le &\sum_{i=1}^n b_i^2 (2\delta + \delta^2)
    \\
    \le &3 \max(\delta, \delta^2) \sum_{i=1}^n b_i^2
    \\
    = &3 \max(\delta, \delta^2).
  \end{align*}
\end{proof}

\begin{customExercise}{8}
  Canonical examples of isomteries and projections can be constructed from a fixed unitary matrix $U$.
  Check that any sub-matrix of $U$ obtained by selecting a subset of columns is an isometry and any sub-matrix obtained by selecting a subset of rows is a projection.
\end{customExercise}
\begin{proof}
  Unitary matrix satisfy $U^\intercal U = U U^\intercal = I_n$.
  So taking $k$ columns will have $\hat{U}^\intercal \hat{U} = I_k$ which shows that it is isometry by Exercise 4.1.4, and taking $k$ rows will have $\hat{U} \hat{U}^\intercal = I_k$ which is again projection by Exercise 4.1.4.
\end{proof}

\setcounter{subsection}{2}
\begin{customExercise}{5}
  \begin{enumerate}
    \item Suppose that $T$ is a normed space. Prove that $\cP(K, d, \epsilon)$ is the largest number of closed disjoint balls with centers in $K$ and radii $\epsilon/2$. 
    \item Show by example that the previous statement may be false for a general metric space $T$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Let's write second number as $B(K, d, \epsilon)$.
    We can first show that $\cP(K, d, \epsilon) \le B(K, d, \epsilon/2)$, by taking the same points as centers.
    If we set another point $x$ that is contained in $B(x, \epsilon/2)$ and $B(y, \epsilon/2)$ for such centers, by triangle inequality, $d(x, y) < \epsilon$ which contradicts the assumption.
    
    For the converse, the normed space assumption is essential.
    We take same points, and now, for $x, y$ that are centers of ball with radius $\epsilon/2$ achieving maximum $B(K, d, \epsilon/2)$, define two points $v, w$ that are
    \[
      v = x + (y - x) \times \frac{\epsilon/2}{\|y-x\|}, \qquad w = y - (y - x) \times \frac{\epsilon/2}{\|y-x\|}.
    \]
    By construction, these points are on single line, in the order of $x, v, w, y$, otherwise it violates the ball assumptions on $x, y$.

    Then, 
    \[
      d(x, y) = d(v, y) \times \frac{\epsilon + d(v, w)}{\epsilon/2} = \epsilon + t > \epsilon.
    \]
    \item This is simple by using the discrete space. For simplicity, we limit $X$ to be finite set, and define metric as
    \[
      d(x, y) = \begin{cases}
        1 & \text{if } x \neq y,
        \\
        0 & \text{if } x = y.
      \end{cases}
    \]
    Then, for $\epsilon = 1.6$, $\cP(K, d, 1.6) = 1$ since we can pick only one point, but $B(K, d, 0.8) = |X|$.
  \end{enumerate}
\end{proof}

\begin{customExercise}{9}
  In our definition of the covering numbers of $K$, we required that the centers $x_i$ of the balls $B(x_i, \epsilon)$ that form a covering lie in $K$. 
  Relaxing this condition, define the exterior covering number $\cN^{ext}(K, d, \epsilon)$ similarly but without requiring that $x_i \in K$.
  Prove that
  \[
    \cN^{ext}(K, d, \epsilon) \le \cN(K, d, \epsilon) \le \cN^{ext}(K, d, \epsilon/2).
  \]
\end{customExercise}
\begin{proof}
  Lower bound is simple, we can choose same point that gives $\cN(K, d, \epsilon)$, which will be greater than or equal to minimum.
  
  For the upper bound, let $\cN = \{x_i\}_{i=1}^N$ be the external $\epsilon/2$-net of $K$.
  For $i = 1, \ldots, N$, we will construct $\epsilon$-net of $K$ by
  choosing $y_i \in K \cap B(x_i, \epsilon/2)$. 
  If such point do not exists or it is one already picked earlier, we can discard that point from $\cN$ also, while still being external $\epsilon/2$-net.

  By this construction, we obtain $\{y_i\}_{i=1}^N$.
  To show that this set is $\epsilon$-net of $K$, fix $x \in K$.
  Then for some $x_i$, $d(x, x_i) \le \epsilon/2$, so by triangle inequality,
  \[
    d(y_i, x) \le d(y_i, x_i) + d(x_i, x) \le \epsilon.
  \]
\end{proof}

\begin{customExercise}{10}
  Give a counterexample to the following monotonicity property:
  \[
    L \subset K \quad\text{implies}\quad \cN(L, d, \epsilon) \le \cN(K, d, \epsilon).
  \]
  Prove an approximate version of monotonicity:
  \[
    L \subset K \quad\text{implies}\quad \cN(L, d, \epsilon) \le \cN(K, d, \epsilon/2).
  \]
\end{customExercise}
\begin{proof}
  First, choose $L = \{0, 1, 2\}$ and $K = \{0, 0.5, 1, 1.5, 2\}$ with $\epsilon = 0.6$. 
  Then $\cN(L, d, 0.6) = 3$ but $\cN(K, d, 0.6) = 2$.

  We can do similar as we've done in proof of Exercise 4.2.9.
  For $\epsilon/2-net$ of $K$ $\cN = \{x_i\}_{i=1}^N$, we construct set as $y_i \in L \cap B(x_i, \epsilon/2)$, and skipping the indices that this set is empty.
  Then the set $\{y_i\}_{i=1}^N$ will have size less than or equal to $N$, and is $\epsilon$-net of $L$, since for any $x \in L$
  \[ I
    d(x, y_i) \le d(x, x_i) + d(x_i, y_i) \le \epsilon.
  \]
\end{proof}

\begin{customExercise}{15}
  Check that the Hamming distance $d_H$ is indeed a metric.
\end{customExercise}
\begin{proof}
  First three properties, $d(x, x) = 0$, $d(x, y) > 0$ for $x \neq y$, and $d(x, y) = d(y, x)$ is immediate.
  The triangle inequality can be proven as
  \begin{align*}
    \{i : x(i) \neq z(i)\} &= \{i : x(i) = y(i) \neq z(i)\} \cup \{i : x(i) \neq y(i) = z(i)\}
    \\
    &\subseteq \{i : y(i) \neq z(i)\} \cup \{i : x(i) \neq y(i)\}.
  \end{align*}
\end{proof}

\begin{customExercise}{16}
  Let $K = \{0, 1\}^n$. 
  Prove that, for every integer $m \in [0, n]$, we have
  \[
    \frac{2^n}{\sum_{k=0}^m \binom{n}{k}} \le \cN(K, d_H, m) \le \mathcal{P}(K, d_H, m) \le \frac{2^n}{\sum_{k=0}^{\lfloor m/2 \rfloor} \binom{n}{k}}.
  \]
\end{customExercise}
\begin{proof}
  We will use Proposition 4.2.12 with counting measure.
  Then, the size $|m B_2^n|$ is
  \begin{align*}
    |m B_2^n| &= \left| \{x : |\{i : x(i) = 1\}| \le m \}\right|
    \\
    &= \left| \sqcup_{k=0}^m\{x : |\{i : x(i) = 1\}| = k\} \right|
    \\
    &= \sum_{k=0}^m \binom{n}{k}.
  \end{align*}
  And the $|m/2 B_2^n|$ is simliar, except we use $\lfloor m/2 \rfloor$.
\end{proof}

\setcounter{subsection}{3}

\begin{customExercise}{7}
  \begin{enumerate}
    \item Prove the converse to the statement of Lemma 4.3.4.
    \item Deduce a converse to Theorem 4.3.5. Conclude that for any error correcting code that encodes $k$-bit strings into $n$-bit strings and can correct $r$ errors, the rate must be
    \[
      R \le 1 - f(\delta)
    \]
    $R = k/n$, $\delta = r/n$, and $f(t) = t \log_2(e/t)$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Suppose we have encoder $E$ and decoder $D$. Let's the points $E(\{0, 1\}^k) \subset \{0, 1\}^n$, and it is enough to show that these points have distance larger than $2r$.
    Suppose not, so that there exist $x, y \in \{0, 1\}^k$, $d(E(x), E(y)) \le 2r$.
    Then we can pick some $t \in \{0, 1\}^n$ such that
    \[d(t, E(x)), d(t, E(y)) \le r.\]
    But this contradicts our assumption on $D$, because $D(t)$ should be equal to $x$ and $y$ simultaneously.
    So our assumption is false, and $d(E(x), E(y)) > 2r$ for all $x, y$, and $E(\{0, 1\}^k)$ is $2r$-separating set and gives lower bound of $2^k$.
    \item We have 
    \[\log_2 \mathcal{P}(\{0, 1\}^n, d_H, 2r) \ge k\]
    by (a).
    And from Exercise 4.2.16, we have
    \[
      \mathcal{P}(\{0, 1\}^n, d_H, 2r) \le \frac{2^n}{\sum_{k=0}^r \binom{n}{k}} \le \frac{2^n}{\binom{n}{r}} \le \frac{2^n}{(n/r)^r}.
    \]
    So, taking logarithm gives
    \[
      k \le n - r \log_2 \frac{r}{n}
    \]
    and
    \[
      R \le 1 - \delta \log_2 \frac{1}{\delta}.
    \]
    To achieve the statement, we need
    \[
      k \le n - r \log_2 \frac{en}{r}.
    \]
    However, using same procedure, we should prove
    \[
      \sum_{k=0}^r \binom{n}{k} \ge \left( \frac{en}{r} \right)^r.
    \]
    which, in fact, the inverse inequality $\le$ holds.
  \end{enumerate}
\end{proof}

\setcounter{subsection}{4}

\begin{customExercise}{2}
  Let $x \in \R^n$ and let $\cN$ be an $\epsilon$-net of the sphere $S^{n-1}$. 
  Show that
  \[
    \sup_{y \in \cN} \langle x, y\rangle \le \|x\|_2 \le \frac{1}{1-\epsilon} \sup_{y \in \cN} \langle x, y\rangle.
  \]
\end{customExercise}
\begin{proof}
  First inequality is immediate since 
  \[
    \|x\|_2 = \sup_{y \in S^{n-1}} \langle x, y\rangle.
  \]
  For the second inequality, for some $z \in S^{n-1}$ and $\|y - z\| \le \epsilon$,
  \begin{align*}
    \langle x, z\rangle &= \langle x, y\rangle + \langle x, z-y\rangle
    \\
    &\le \langle x, y\rangle + \epsilon \|x\|_2
  \end{align*}
  Then taking supremum over $z$ shows that
  \[
    \|x\|_2 \le \sup_{y \in \cN} \langle x, y \rangle + \epsilon \|x\|_2.
  \]
  Rearranging this proves the result.
\end{proof}

\begin{customExercise}{3}
  Let $A$ be an $m \times n$ matrix and $\epsilon \in [0, 1/2)$.
  \begin{enumerate}
    \item Show that, for any $\epsilon$-net of the sphere $S^{n-1}$ and any $\epsilon$-net $\cM$ of the sphere $S^{m-1}$, we have
    \[
      \sup_{x \in \cN, y \in cM} \langle Ax, y\rangle \le \|A\| \le \frac{1}{1-2\epsilon} \sup_{x \in \cN, y \in \cM} \langle Ax, y\rangle.
    \]
    \item Moreover, if $m=n$ and $A$ is symmetric, show that
    \[
      \sup_{x \in \cN} |\langle Ax, x\rangle| \le \|A\| \le \frac{1}{1-2\epsilon} \sup_{x \in \cN} |\langle Ax , x\rangle|.
    \]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item For any $v \in S^{n-1}$ and $w \in S^{m-1}$, and $\|v-y\|, \|w-z\| \le \epsilon$,
    \begin{align*}
      \langle Av, w\rangle &= \langle Ay, z\rangle + \langle A(v-y), z\rangle + \langle Av, w-z\rangle
      \\
      &\le \langle Ay, z\rangle + \|A\| \epsilon + \|A\| \epsilon
      \\
      &= \langle Ay, z\rangle + 2\epsilon \|A\|.
    \end{align*}
    So taking supremum over $v, w$, we obtain
    \[
      \|A\| \le \sup_{y\in \cN, z \in \cM} \langle Ay, z\rangle+ 2 \epsilon \|A\|.
    \]
    \item We have similar approach, for any $x \in S^{n-1}$ and $\|x-y\| \le \epsilon$,
    \begin{align*}
      \langle Ax, x\rangle &= \langle Ay, y\rangle + \langle A(x-y), y\rangle + \langle Ax, x-y\rangle
      \\
      &\le \langle Ay, y\rangle + \epsilon \|A\| + \epsilon \|A\|
      \\
      &= \langle Ay, y\rangle + 2\epsilon \|A\|.
    \end{align*}
  \end{enumerate}
\end{proof}

\begin{customExercise}{4}
  Let $A$ be an $m \times n$ matrix, $\mu \in \R$ and $\epsilon \in [0, 1/2)$.
  Show that for any $\epsilon$-net $\cN$ of the sphere $S^{n-1}$, we have
  \[
    \sup_{x \in S^{n-1}} |\|Ax\|_2 - \mu| \le \frac{C}{1-2\epsilon} \sup_{x \in \cN} |\|Ax\|_2 - \mu|.
  \]
\end{customExercise}
\begin{proof}
  Suppose $\mu \le 0$. 
  Then LHS is immediately $\|A\| + \mu$, and applying Lemma 4.4.1 gives
  \[
    \sup_{x \in \cN} |\|Ax\|_2 - \mu| = \sup_{x \in \cN} \|Ax\|_2 - \mu \le \frac{1}{1-\epsilon} (\|A\| + \mu).
  \]
  So the inequality holds with $C = 1$, if $\mu \le 0$.

  Now let's split the case by $\|A\| \le M$ or $\|A\| > M$.
  Using the inequality $|t-1|\le |t^2 - 1|$ for $t > 0$, we can write
  \begin{align*}
    \sup_{x \in \R^n} |\|Ax\|_2 - 1| &\le \sup_{x \in \R^n} |\|Ax\|_2^2 - 1|
    \\
    &= \sup_{x \in \R^n} |\langle A^\intercal Ax, x\rangle - \langle x, x\rangle|
    \\
    &= \sup_{x \in \R^n} \langle (A^\intercal A - I)x, x\rangle
    \\
    &\le \frac{1}{1-\epsilon} \sup_{x \in \cN} \langle (A^\intercal A - I)x, x\rangle
    \\
    &= \frac{1}{1-\epsilon} \sup_{x \in \cN} |\|Ax\|_2^2 - 1|
    \\
    &= \frac{1}{1-\epsilon} \sup_{x \in \cN} |\|Ax\|_2 - 1| (\|Ax\|_2 + 1)
    \\
    &\le \frac{M+1}{1-\epsilon} \sup_{x \in \cN} |\|Ax\|_2 - 1|.
  \end{align*}
  So we prove the inequality for this case. 

  Now if $\|A\| \ge 2$, the maximum is attained with $\|A\|_2 - 1$, with $x^*$ with $\|x^* - y\|_2 < \epsilon$.
  Then, 
  \begin{align}
    \sup_{x \in \cN} |\|Ax\|_2 - 1| &\ge \|Ay\|_2 - 1 \nonumber
    \\
    &\ge \|Ax^*\| - \|A (x^* - y)\| - 1 \nonumber
    \\
    &\ge \|A\| (1 - \epsilon) - 1 \label{eqn:mid}
  \end{align}
  and rearranging it gives
  \[
    \|A\| -1 \le \frac{1}{1-\epsilon} \left( \sup_{x \in \cN} |\|Ax\|_2 - 1| + 1\right) - 1 = \frac{1}{1-\epsilon} \left( \sup_{x \in \cN} |\|Ax\|_2 - 1| + \epsilon \right).
  \]
  If we write the supremum as $T$, we need to show that
  \[
    \frac{T + \epsilon}{1-\epsilon} \le \frac{C T}{1-2\epsilon},
  \]
  while $T > 1 - 2\epsilon$ by plugging in our assumption \ref{eqn:mid}.
  \[
    2 (C-3) \epsilon^2 - (3C - 5) \epsilon + C-1 \ge 0
  \]
  which holds if $C \ge 1$.

  Summing up, we can set $M=2$ to have $C=3$.
\end{proof}

\begin{customExercise}{6}
  Deduce from Theorem 4.4.5 that
  \[
    \E \|A\| \le C K (\sqrt{m} + \sqrt{n}).
  \]
\end{customExercise}
\begin{proof}
  \begin{align*}
    \E \|A\| &\le CK(\sqrt{m} + \sqrt{n}) + \int_{t=CK(\sqrt{m}+\sqrt{n})}^\infty P(\|A\| > t) dt
    \\
    &\le CK(\sqrt{m} + \sqrt{n}) + \int_{t=CK(\sqrt{m}+\sqrt{n})}^\infty 2\exp(-t^2) dt
    \\
    &\le CK (\sqrt{m} + \sqrt{n}) + \sqrt{\pi}.
  \end{align*}
\end{proof}

\begin{customExercise}{7}
  Suppose that in Theorem 4.4.5 the entries $A_{ij}$ have unit variances.
  Prove that
  \[
    \E \|A\| \ge C (\sqrt{m} + \sqrt{n}).
  \]
\end{customExercise}
\begin{proof}
  We first have that
  \begin{align*}
    \|A\| &\ge \|A_{:,1}\|,
    \\
    \|A\| &\ge \|A_{1,:}\|
  \end{align*}
  which uses $\|A\| \ge \|A e_1\|$ and $\|A\| = \|A^\intercal\| \ge \|A^\intercal e_1\|$.

  By Exercise 3.1.4(b), this shows
  \begin{align*}
    \E \|A\| &\ge \max(\sqrt{m} - o(1), \sqrt{n} - o(1))
    \\
    &\ge \frac{1}{2} (\sqrt{m} + \sqrt{n} - o(1)).
  \end{align*}
  And we can take $C$ small enough so that $o(1)$ can be ignored.
\end{proof}

\setcounter{subsection}{5}
\begin{customExercise}{2}
  Check that the matrix $D$ has rank 2 and that the nonzero eigenvalue $\lambda_i$ and the corresponding eigenvectors $u_i$ are
  \begin{align*}
    \lambda_1 &= \left( \frac{p + q}{2} \right) n, & u_1 &= (1, \cdots, 1, 1, \cdots, 1)^\intercal,
    \\
    \lambda_2 &= \left( \frac{p-q}{2} \right)n, & u_2 &= (1, \cdots, 1, -1, \cdots, -1)^\intercal.
  \end{align*}
\end{customExercise}
\begin{proof}
  We can rewrite $D$ as
  \[
    D = \frac{p+q}{2} u_1 u_1^\intercal + \frac{p-q}{2} u_2 u_2^\intercal
  \]
  so $D$ is rank 2.

  And both $\|u_1\| = \|u_2\| = n$, so we have the correct eigenvalue.
\end{proof}

\begin{customExercise}{4}
  Deduce Weyl's inequality from the Courant-Fisher min-max characterization of eigenvalues.
\end{customExercise}
\begin{proof}
  We first note that
  \begin{align*}
    \lambda_i(S) &= \max_{\dim E = i} \min_{x \in S(E)} \langle Sx, x\rangle,
    \\
    \lambda_i(T) &= \max_{\dim F = i} \min_{y \in S(F)} \langle Ty, y\rangle.
  \end{align*}
  If we have $E^*, F^*$ that attain the maximum, and $x^*$ and $y^*$ the basis having the minimum.
  Let's apply the change of basis so that $x^* = y^*$, which gives us
  \[
    |\lambda_i(S) - \lambda_i(T)| = |\langle Sx^*, x^* \rangle - \langle Tx^*, x^*\rangle| \le |\langle (S-T) x^*, x^* \rangle| \le \| S-T\|.
  \]
\end{proof}

\setcounter{subsection}{6}

\begin{customExercise}{2}
  Deduce from (4.22) that
  \[
    \E \left\| \frac{1}{m} A^\intercal A - I_n \right\| \le C K^2 \left( \sqrt{\frac{n}{m}} + \frac{n}{m} \right).
  \] 
\end{customExercise}
\begin{proof}
  Let's write $X$ as the operator norm we are concerned.
  \begin{align*}
    \E X &= \int_0^\infty P(X > t) dt
    \\
    &\le M + \int_0^\infty P(X > M + t) dt
  \end{align*}
  where 
  \[
    M = K^2 \left( 2C \sqrt{\frac{n}{m}} + 4 C^2 \frac{n}{m}\right),
  \] 
  which can be obatined by plugging in $t = \sqrt{n}$ for $K^2 \max(\delta, \delta^2)$.

  Now for $P(X > M + t)$, we know that
  \begin{align*}
    P\left( X > M + K^2 \left( \frac{C \sqrt{m}s + C^2 s^2}{m} \right) \right) \le 2\exp(-s^2)
  \end{align*}
  Since we already covered case when $s \le \sqrt{n}$, we may assume $s > \sqrt{n}$ and $s^2 \ge s$, so
  \[
    P\left( X > M + K^2 \left( \frac{C \sqrt{m} + C^2}{m} \right) s^2 \right) \le 2\exp(-s^2)
  \]
  and rearranging this gives
  \[
    P\left( X > M + t \right) \le 2\exp\left( - \frac{t}{K^2 \left( \frac{C \sqrt{m} + C^2}{m} \right)} \right)
  \]
  and
  \begin{align*}
    \E X &\le M + \int_0^\infty 2\exp\left( - \frac{t}{K^2 \left( \frac{C \sqrt{m} + C^2}{m} \right)} \right)dt
    \\
    &= K^2 \left( 2C \sqrt{\frac{n}{m}} + 4 C^2 \frac{n}{m}\right) + 2 K^2 \left( \frac{C\sqrt{m} + C^2}{m} \right)
    \\
    &= K^2 \left( \frac{2C\sqrt{n} + 2}{\sqrt{m}} + \frac{4C^2 n + 2C^2}{m} \right)
    \\
    &\le K^2 \left( 4C\sqrt{\frac{n}{m}} + 6C^2 \frac{n}{m} \right).
  \end{align*}

\end{proof}

\begin{customExercise}{3}
  Deduce from Theorem 4.6.1 the following bounds on the expectation;
  \[
    \sqrt{m} - C K^2 \sqrt{n} \le \E s_n(A) \le \E s_1(A) \le \sqrt{m} + CK^2 \sqrt{n}.
  \]
\end{customExercise}
\begin{proof}
  \begin{align*}
    \E s_1(A) &\le \sqrt{m} + CK^2 \sqrt{n} + \int_0^\infty P(s_1(A) > \sqrt{m} + CK^2 \sqrt{n} + t) dt
    \\
    &\le \sqrt{m} + CK^2 \sqrt{n} + 2 \int_0^\infty \exp\left( - \frac{t^2}{C^2 K^4} \right) dt
    \\
    &= \sqrt{m} + CK^2 \sqrt{n} + \sqrt{\pi} C K^2.
  \end{align*}
  The lower bound is similar.
\end{proof}

\begin{customExercise}{4}
  Give a simpler proof of Theorem 4.6.1, using Theorem 3.1.1 to obtain a concentration bounds for $\|Ax\|_2$ and Exercise 4.4.4 to reduce to a union bound over a net.
\end{customExercise}
\begin{proof}
  By assumption, for any vector $x$, $\langle A_i, x\rangle$ is independent mean-zero $K$-sub-gaussians.
  This means $A x$ is a random vector of independent coordinates, with sub-gaussian coordinates of norm $K$.

  Now Lemma 3.1.1 shows that
  \[
    \left\| \|Ax\|_2 - \sqrt{m} \right\|_{\psi_2} \le CK^2.
  \]
  By Sub-gaussian tail probability, this implies that
  \begin{align*}
    P\left( \left| \|Ax\|_2 - \sqrt{m} \right| > t \right) \le 2\exp\left( -\frac{Ct^2}{K^4} \right).
  \end{align*}
  By Exercise 4.4.4, 
  \begin{align*}
    &P\left( \sup_{\|x\| = 1} \left| \|Ax\|_2 - \sqrt{m} \right| > t \right)
    \\
    \le &P\left( 2c \sup_{x \in \cN_{1/4}} \left| \|Ax\|_2 - \sqrt{m} \right| > t \right)
    \\
    \le &9^n P\left( \left| \|Ax\|_2 - \sqrt{m} \right| > \frac{t}{2c} \right)
    \\
    \le &2 \cdot 9^n \cdot \exp\left( - \frac{C t^2}{4K^4 c^2} \right)
    \\
    = &2 \cdot \exp\left(n \log 9 - \frac{C t^2}{4K^4 c^2} \right).
  \end{align*}
  So this implies
  \begin{align*}
    &P\left( \sup_{\|x\| = 1} \left| \|Ax\|_2 - \sqrt{m} \right| > CK^2 \sqrt{n} + CK^2t \right)
    \\
    \le &2 \exp\left( n \log 9 - \frac{C_1 C^2 K^4 (t + \sqrt{n})^2 }{K^4}\right)
    \\
    \le &2 \exp\left( n \log 9 - C_1 C^2 (t^2 + n)\right)
    \\
    \le &2 \exp\left( - C t^2 \right).
  \end{align*}
  which is achieved by setting $C$ large enough, so that $C_1 C^2 > \log 9$. 
\end{proof}

\setcounter{subsection}{7}

\begin{customExercise}{3}
  Our argument also implies the following high-probability guarantee. 
  Check that for any $u \ge 0$, we have
  \[
    \|\Sigma_m - \Sigma\| \le CK^2 \left( \sqrt{\frac{n + u}{m}} + \frac{n + u}{m} \right) \|\Sigma\|
  \]  
  with probability at least $1 - 2 \exp(-u)$.
\end{customExercise}
\begin{proof}
  By Theorem 4.6.1, we can show that
  \[
    \left\|R_m\right\| \le C K^2 \left( \delta + \delta^2 \right)
  \]
  with probability at least $1 - 2 \exp(-t^2)$.
  Using $u = t^2$, we have 
  \begin{align*}
    \left\|R_m\right\| &\le C K^2 \left( \frac{\sqrt{n} + \sqrt{u}}{\sqrt{m}} + \frac{n + u + 2 \sqrt{nu}}{m} \right)
    \\
    &\le C K^2 \left( \frac{\sqrt{2(n + u)}}{\sqrt{m}} + \frac{2n + 2u}{m} \right)
    \\
    &\le 2C K^2 \left( \frac{\sqrt{(n + u)}}{\sqrt{m}} + \frac{n + u}{m} \right).
  \end{align*}
\end{proof}

\begin{customExercise}{6}
  Prove Theorem 4.7.5 for the spectral clustering algorithm applied to the Gaussian mixture model.
  Proceed as follows.
  \begin{enumerate}
    \item Compute the covariance matrix $\Sigma$ of $X$; note that the eigenvector corresponding to the largest eigenvalue is parallel to $\mu$.
    \item Use results about covariance estimation to show that the sample covariance matrix $\Sigma_m$ is close to $\Sigma$ if the sample size $M$ is relateively large.
    \item Use the Davis-Kahan theorem 4.5.5 to deduce that the first eigenvector $v = v_1(\Sigma_m)$ is close to the direction of $\mu$.
    \item Conclude that the signs of the $\langle u, X_i \rangle$ predict well to which community $X_i$ belongs.
    \item Since $v \approx \mu$, conclude the same for $v$.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{align*}
    \Var(X_i) &= \mu_i^2 \Var(\theta) + \Var(g_i)
    \\
    &= \mu_i^2 + 1,
    \\
    \Cov(X_i, X_j) &= \mu_i \mu_j \Var(\theta) + \Cov(g_i, g_j)
    \\
    &= \mu_i \mu_j.
  \end{align*}
  So, 
  \[
    \Cov(X) = \mu \mu^\intercal + I_n
  \]
  and the largest eigenvalue $\|\mu\| + 1$ corresponds to unit vector $\mu / \|\mu\|$, and all the rest of eigenvalues are 1.

  Now note that
  \[
    \langle X, x\rangle \sim \Theta \langle \mu, x\rangle + \cN(0, 1)
  \]
  for any unit vector.
  By Exercise 2.7.11, we have
  \[
    \|\langle X, x\rangle\|_{\psi_2} \le \frac{\langle \mu, x\rangle}{\sqrt{\ln 2}} + C
  \]
  while 
  \[
    \|\langle X, x\rangle\|_{L_2} = \sqrt{\langle \Sigma x, x\rangle} = \sqrt{\langle \mu, x\rangle^2 + \|x\|^2} \ge \frac{\langle \mu, x\rangle + 1}{\sqrt{2}}.
  \]
  So we satisfy the condition of Exercise 4.7.3 with
  \[
    K = \max\left( \sqrt{\frac{\ln 2}{2}}, \frac{1}{\sqrt{2} C} \right).
  \]
  and have the tail bound as
  \[
    \left\|\Sigma_m - \Sigma\right\| \le C \left( \sqrt{\frac{n + u}{m}} + \frac{n+u}{m}\right) \|\Sigma\|
  \]
  with probability at least $1 - 2 \exp(-u)$.
  Note that $\|\Sigma\| = \|\mu\| + 1$.

  Plugging in $u = n$, we have
  \[
    \|\Sigma_m -\Sigma\| \le C \left(\sqrt{n/m} + n/m\right)(\|\mu\| + 1)
  \]
  with probability $\ge 1 - 2\exp(-n)$.

  And Theorem 4.5.5 shows that
  \[
    \|v_1(\Sigma_m) - \bar{\mu}\| \le \frac{2^{3/2} \|\Sigma_m - \Sigma\|}{\|\mu\|} = \frac{C (\|\mu\| + 1)}{\|\mu\|} \left( \sqrt{n/m} + n/m\right).
  \]
  We can use this to bound the inner product as 
  \begin{align*}
    \langle v_1(\Sigma_m), \bar{\mu} \rangle &= \|\bar{\mu}\|^2 + \langle v_1(\Sigma_m) - \bar{\mu}, \bar{\mu} \rangle
    \\
    &\ge 1 - C\frac{\|\mu\| + 1}{\|\mu\|}\left( \sqrt{n/m} + n/m\right).
  \end{align*}
  that holds with probability $\ge 1 - 2 \exp(-n)$.

  Since random guess already have $0.5m$ correctness, we can assume $\epsilon < 0.5$, and can set $C$ in the assumption of $\|\mu\|_2$ large enough so that $\|\mu\| \ge 1$. 
  This allows to bound as
  \[
    \langle v_1, \bar{\mu}\rangle \ge 1 - C \left(\sqrt{n/m} + n/m \right)
  \]
  What we will be interested in is square of it, so we will bound it as
  \begin{align*}
    \langle v_1, \bar{\mu}\rangle^2 &\ge 1 - 2 C \left(\sqrt{n/m} + n/m \right) + \left(\sqrt{n/m} + n/m\right)^2
    \\
    &\ge 1 - 2 C \left(\sqrt{n/m} + n/m\right).
  \end{align*}


  Now the probability of misclassification can be written as
  \begin{align*}
    P\left( \Theta \langle v_1, X\rangle < 0 \right)
    &= P\left( \langle v_1, \mu \rangle + Z < 0 \right)
    \\
    &= P\left( Z < -\langle v_1, \mu \rangle \right)
    \\
    &\le \exp\left( - \frac{\langle v_1, \mu \rangle^2}{2} \right)
    \\
    &\le \exp\left( - \frac{\|\mu\|^2}{2} \cdot \left( 1 - 2C (\sqrt{n/m} + n/m)\right) \right) 
    \\
    &\le \exp\left( - \frac{\|\mu\|^2}{2} \cdot \left( 1 - 2C (\sqrt{\|\mu\|} + \|\mu\|)\right) \right)
  \end{align*}
  where $Z \sim \cN(0, 1)$, assuming that $c = 1$.
  Let's write this probability $p$.
  Then the probability of having more than $\epsilon m$ classification is bounded as
  \begin{align*}
    P(T > \epsilon m) \le \binom{m}{\epsilon m} p^{\epsilon m} \le \left(\frac{ep }{\epsilon}\right)^{\epsilon m}
  \end{align*}
  which gives
  \begin{align*}
    P(T > \epsilon m) &\le \exp\left( \epsilon m \times \left( 1 - \frac{\langle v_1, \mu \rangle^2}{2} - \log \epsilon \right)\right)
    \\
    &\le \exp \left( \epsilon m \times \left( 1 - \|\mu\| \frac{1 - 2 C (\sqrt{n/m} + n/m)}{2} - \log \epsilon\right) \right)
    \\
    &\le \exp\left( \epsilon m \times \left( 1 - \|\mu\| \frac{1 - 2C(\sqrt{\|\mu\|} + \|\mu\|)}{2} - \log \epsilon\right) \right)
  \end{align*}

  TODO.
\end{proof}
\end{document}