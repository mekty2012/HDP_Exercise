\documentclass[a4paper]{article}
\usepackage{amsmath,amssymb,amsthm,tikz,hyperref}
\usepackage{bbm}

\newtheorem{exercise}{Exercise}[subsection]

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\cN}{\mathcal{N}}
\newcommand*{\Var}{\mathrm{Var}}
\newcommand*{\Cov}{\mathrm{Cov}}
\newcommand*{\esssup}{\mathrm{ess}\,\sup}
\newcommand*{\ind}[1]{\mathbbm{1}_{[{#1}]}}
\newcommand*{\defeq}{\stackrel{\mathrm{def}}{=}}

\renewcommand{\theenumi}{(\alph{enumi})}
\makeatletter

\newenvironment{customExercise}[1]
 {\count@\c@exercise
 \global\c@exercise#1 %
   \global\advance\c@exercise\m@ne
   \exercise}
  {\endexercise
  \global\c@exercise\count@}

\title{Random Vectors in High Dimensions}
\author{Taeyoung Kim}
\date{\today}

\begin{document}
\maketitle

\setcounter{section}{3}
\setcounter{subsection}{1}

\begin{customExercise}{4}
  \begin{enumerate}
    \item Deduce from Theorem 3.1.1 that 
    \[
      \sqrt{n} - C K^2 \le \E \|X\|_2 \le \sqrt{n}+ CK^2.
    \]
    \item Can $CK^2$ be replaced by $o(1)$, a quantity that vanishes as $n \to \infty$?
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item We can bound the expectation's deviation as
    \begin{align*}
      \E \|X\|_2 - \sqrt{n} 
      &= \int_0^\infty P(\|X\|_2 - \sqrt{n} \ge t) dt 
      \\
      &\le \int_0^\infty 2 \exp \left( - \frac{ct^2}{K^4} \right) dt
      \\
      &= 2 \frac{\sqrt{\pi} K^2}{\sqrt{c}}
    \end{align*}
    and the lower side also holds.
    \item The upper bound $\E \|X\|_2 \le \sqrt{n}$ is immediate with Jensen's inequality.
    For the lower bound, let's consider the term similar to variance, $\E (\|X\|_2 - \sqrt{n})^2$, which is
    \[
      \E (\|X\|_2 - \sqrt{n})^2 = \E \|X\|_2^2 - 2 \sqrt{n} \E\|X\|_2 + n = 2 (n - \sqrt{n} \E\|X\|_2).
    \]
    Theorem 3.1.1 shows that LHS is bounded, since it is second moment of the sub-gaussian random variable.
    We have
    \[
      \E \|X\|_2 \ge \sqrt{n} - \frac{1}{2\sqrt{n}} CK^4
    \]
    which shows the result.
  \end{enumerate}
\end{proof}

\begin{customExercise}{5}
  Deduce from Theorem 3.1.1 that
  \[
    \mathrm{Var}(\|X\|_2) \le C K^4.
  \]
\end{customExercise}
\begin{proof}
  By centering lemma, we know that $\|\|X\|_2 - \E \|X\|_2 \|_{\psi_2} \le C_1 K^2$.
  Then by the (2) of Proposition 2.7.1, the second moment is bounded as $C_2 K^4$, proving the result.
\end{proof}

\begin{customExercise}{6}
  Let $X = (X_1, \ldots, X_n) \in \R^n$ be a random vector with independent coordinates $X_i$ that satisfy $\E X_i^2 = 1$ and $\E X_i^4 \le K^4$.
  Show that 
  \[
    \mathrm{Var}(\|X\|_2) \le C K^4.
  \]
\end{customExercise}
\begin{proof}
  Let's first see the variance of $\|X\|_2^2$, as
  \begin{align*}
    \E \left[ (\|X\|_2^2 - n)^2 \right] &= \E \left[ \|X\|_2^4 - 2n \|X\|_2^2 + n^2 \right]
    \\
    &= \E \left[ \left(\sum_{i=1}^n X_i^2\right)^2 - 2 n \sum_{i=1}^n X_i^2 + n^2 \right]
    \\
    &= \E \left[ \left(\sum_{i=1}^n X_i^2\right)^2 \right] - n^2
    \\
    &= \E \left[ \sum_{i=1}^n X_i^4 + \sum_{\substack{i,j=1 \\ i \neq j}}^n X_i^2 X_j^2 \right] - n^2
    \\
    &\le n K^4 + n(n-1) - n^2 
    \\
    &= nK^4 - n \le nK^4.
  \end{align*}
  Then we can derive following inequality as
  \begin{align*}
    \E \left[ (\|X\|_2^2 - n)^2 \right] &= \E \left[ (\|X\|_2 - \sqrt{n})^2 (\|X\|_2 + \sqrt{n})^2 \right]
    &\ge n \E \left[ (\|X\|_2 - \sqrt{n})^2 \right]
  \end{align*}
  which gives $\E [\|X\|_2 - \sqrt{n}] \le K^4$.
  Finally, using the inequality of the variance, we can deduce that
  \[
    \E [(\|X\|_2 - \E \|X\|_2 )^2] \le \E [(\|X\|_2 - \sqrt{n})^2] \le K^4.
  \]
\end{proof}

\begin{customExercise}{7}
  Let $X = (X_1, \ldots, X_n) \in \R^n$ be a random vector with independent coordinates $X_i$ having continuous distributions.
  Assume that the densities of $X_i$ are uniformly bounded by 1.
  Show that, for any $\epsilon > 0$, we have
  \[
    P \left( \|X\|_2 \le \epsilon \sqrt{n} \right) \le (C\epsilon)^n.
  \]
\end{customExercise}
\begin{proof}
  We will instead use the following probability
  \[P(\|X\|_2 \le \epsilon \sqrt{n}) = P\left(\sum_{i=1}^n X_i^2 \le \epsilon^2 n \right).\]
  Now if we compute the MGF of $X_i^2$, we have
  \begin{align*}
    \E \exp(- tX^2) &= \int_{\R} p(x) \exp(- tx^2) dx
    \\
    &\le \int_{\R} \exp(-tx^2) dx 
    \\
    &= \frac{\sqrt{\pi}}{\sqrt{t}}.
  \end{align*}
  Then using this bound, we can bound the probability as
  \begin{align*}
    P\left(\sum_{i=1}^n X_i^2 \le \epsilon^2 n \right) &= P \left( -\sum_{i=1}^n t X_i^2 \ge -t \epsilon^2 n \right)
    \\
    &= P\left( \exp\left( - \sum_{i=1}^n t X_i^2 \right) \ge \exp(-t \epsilon^2 n) \right)
    \\
    &\le \exp\left( t \epsilon^2 n \right) \E \left[ \exp\left( - \sum_{i=1}^n t X_i^2 \right) \right]
    \\
    &= \exp(t \epsilon^2 n) \prod_{i=1}^n \E \left[ \exp(- tX_i^2) \right]
    \\
    &\le \exp(t \epsilon^2 n) \left( \frac{\sqrt{\pi}}{\sqrt{t}} \right)^n.  
  \end{align*}
  Setting $t = 1/2\epsilon^2$ gives the minimum, as
  \[
    p \le \exp(n/2) \left( \sqrt{2\pi} \epsilon \right)^n = \left( \sqrt{2 e \pi} \epsilon \right)^n.
  \]
\end{proof}

\setcounter{subsection}{2}

\begin{customExercise}{2}
  \begin{enumerate}
    \item Let $Z$ be an isotropic mean-zero random vector in $\R^n$.
    Let $\mu \in \R^n$ be a fixed vector and $\Sigma$ be a fixed $n\times n$ positive-semidefinite matrix. 
    Check that the random vector
    \[
      X \defeq \mu + \Sigma^{1/2} Z
    \]
    has mean $\mu$ and covariance matrix $\mathrm{cov}(X) = \Sigma$.
    \item Let $X$ be a random vector with invertible covariance matrix $\Sigma = \mathrm{cov}(X)$. 
    Check that the random vector 
    \[
      Z \defeq \Sigma^{-1/2} (X - \mu)
    \]
    is an isotropic mean-zero random vector.
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item The mean is straightforward with the linearity of expectation, 
    \[
      \E X = \E \mu + \Sigma^{1/2} \E Z = \mu.
    \]
    For the covariance, we have 
    \begin{align*}
      \mathrm{cov}(X) &= \E \left[ \Sigma^{1/2} Z Z^\intercal \left(\Sigma^{1/2}\right)^\intercal \right]
      \\
      &= \Sigma^{1/2}\E \left[ ZZ^\intercal  \right]\left(\Sigma^{1/2} \right)^\intercal
      \\
      &= \Sigma.
    \end{align*}
    \item Again for the mean, we have
    \[
      \E Z = \Sigma^{-1/2} (\E X - \mu) = 0.
    \]
    And the covariance have
    \begin{align*}
      \E ZZ^\intercal &= \E \left[ \Sigma^{-1/2} (X - \mu) (X - \mu)^\intercal \left(\Sigma^{-1/2}\right)^\intercal \right]
      \\
      &= \Sigma^{-1/2} \E \left[ (X - \mu) ( X - \mu)^\intercal \right] \left( \Sigma^{-1/2} \right)^\intercal
      \\
      &= \Sigma^{-1/2} \Sigma \left(\Sigma^{-1/2}\right)^\intercal
      \\
      &= I.
    \end{align*}
  \end{enumerate}
\end{proof}

\begin{customExercise}{6}
  Let $X$ and $Y$ be independent mean-zero isotropic random vectors in $\R^n$.
  Check that 
  \[
    \E\|X - Y\|_2^2 = 2n.
  \]
\end{customExercise}
\begin{proof}
  \begin{align*}
    \E \|X - Y\|_2^2 
    &= \E \left[ (X - Y)^\intercal (X-Y) \right]
    \\
    &= \E \left[ \mathrm{tr} \left((X - Y)^\intercal (X-Y)\right) \right]
    \\
    &= \E \left[ \mathrm{tr}\left( X^\intercal X - X^\intercal Y - Y^\intercal X + Y^\intercal Y \right) \right]
    \\
    &= n - 0 - 0 + n.
  \end{align*}
\end{proof}

\setcounter{subsection}{3}

\begin{customExercise}{1}
  Show that the spherically distributed random vector $X$ is isotropic. 
  Argue that the coordinates of $X$ are not independent.
\end{customExercise}
\begin{proof}
  We will show two things, that $\E X_i^2 = 1$ and $\E X_i X_j = 0$ for $i \neq j$.
  For the variance, using the fact that $n$-dimension ball's surface area is proportional to $R^{n-1}$, we can show that 
  \begin{align*}
    \E X_i^2 &= \int_{-\sqrt{n}}^{\sqrt{n}} x^2 p(X_i = x) dx
    \\
    &= \frac{\int_{-\sqrt{n}}^{\sqrt{n}} x^2 S_{n-1}(\sqrt{n - x^2}) (\sqrt{n} / \sqrt{n - x^2}) dx}{\int_{-\sqrt{n}}^{\sqrt{n}} S_{n-1}(\sqrt{n - x^2}) (\sqrt{n} / \sqrt{n - x^2}) dx}
    \\
    &= \frac{\int_{-\pi/2}^{\pi/2} (\sqrt{n} \sin \theta)^2 (\sqrt{n} \cos \theta)^{n-2} d\theta}{\int_{-\pi/2}^{\pi/2} (\sqrt{n} \cos \theta)^{n-2} d\theta}
    \\
    &= n \times \frac{\int_{-\pi/2}^{\pi/2} \cos^{n-2} \theta- \cos^n \theta d\theta }{\int_{-\pi/2}^{\pi/2} \cos^{n-2} \theta d\theta}
    \\
    &= n \left( 1 - \frac{\sqrt{\pi} \Gamma(n/2+1/2) / \Gamma(n/2+1)}{\sqrt{\pi} \Gamma(n/2-1/2) / \Gamma(n/2)} \right)
    \\
    &= n \left( 1 - \frac{n-1}{n} \right)
    \\
    &= 1
  \end{align*}
  where $S_{n}(r)$ is the surface area of $n$-ball, and is proportional to $r^{n-1}$.

  Now for the covariance, 
  \[
    \E X_i X_j = \E_{X_i} [\E_{X_j} [X_j | x_i]] = 0.
  \]
  where we used the fact that when conditioned on one coordinates, other coordinates are symmetric.

  Finally for the independency, we can see that 
  \begin{align*}
    P \left(X_i > \sqrt{\frac{n}{2}}\right) &> 0 \\
    P \left(X_i > \sqrt{\frac{n}{2} \wedge X_j > \sqrt{\frac{n}{2}}}\right) &= 0.
  \end{align*}
\end{proof}

\begin{customExercise}{3}
  \begin{enumerate}
    \item Consider a random vector $g \sim \cN(0, I_n)$ and a fixed vector $u \in \R^n$. Then
    \[
      \langle g, u \rangle \sim \cN(0, \|u\|_2^2).
    \]
    \item Consider independent random variables $X_i \sim \cN(0, \sigma_i^2)$. Then
    \[
      \sum_{i=1}^n X_i \sim \cN\left( 0, \sum_{i=1}^n \sigma_i^2 \right).
    \]
    \item Let $G$ be an $m \times n$ Gaussian random matrix, i.e., the entries of $G$ are independent $\cN(0, 1)$ random variables. 
    Let $u \in \R^n$ be a fixed unit vector. Then 
    \[
      Gu \sim \cN(0, I_m).
    \]
  \end{enumerate}
\end{customExercise}
\begin{proof}
  \begin{enumerate}
    \item Let's write $U$ be the orthogonal matrix with its first row $u / \|u\|$, and rest of the rows fill the orthogonal spaces.
    Then we can rewrite the inner product as 
    \[
      \langle Ug, u\rangle = \langle \Pi_u g, u \rangle + \langle \Pi_u^\perp g, u\rangle = \langle \Pi_u g, u \rangle
    \]
    and has distribution $\|u\| \cN(0, 1) = \cN(0, \|u\|^2)$.
    \item We will use (a), by setting $u$ be the vector $(\sigma_1, \ldots, \sigma_n)$. 
    Then $\langle g, u\rangle$ has each summand of inner product as $X_i \sim \cN(0, \sigma_i^2)$, and the summation has the variance $\|u\|_2^2$.
    \item Let's vectorize $G$ to $mn$ vector, by stacking the rows.
    Then define the orthogonal matrix $U \in \R^{mn \times mn}$ whose first $m$ rows are defined by placing $u$ on $m \times (i- 1) + 1$ to $mi$ coordinates, and filling the rest of the rows.
    Then the first $m$ coordinates are same distribution as $Gu$, so this proves the result.
  \end{enumerate}
\end{proof}

\begin{customExercise}{4}
  Let $X$ be a random vector in $\R^n$.
  Show that $X$ has a multivariate normal distribution if and only if every one-dimensional marginal $\langle X, \theta \rangle$, $\theta \in \R^n$, has a (univariate) normal distribution.
\end{customExercise}
\begin{proof}
  
\end{proof}
\end{document}
